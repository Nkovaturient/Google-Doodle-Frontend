{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nkovaturient/Google-Doodle-Frontend/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.0 Flash\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://goo.gle/40JXy6g\">\n",
        "      <img width=\"32px\" src=\"https://cdn.qwiklabs.com/assets/gcp_cloud-e3a77215f0b8bfa9b3f611c0d2208c7e8708ed31.svg\" alt=\"Google Cloud logo\"><br> Open in Cloud Skills Boost\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) |  [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is available through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
        "\n",
        "- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
        "- Speed and performance: Gemini 2.0 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.\n",
        "- Quality: The model maintains quality comparable to larger models like Gemini 1.5 Pro and GPT-4o.\n",
        "- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
        "- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.\n",
        "- To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API in Vertex AI and the Google Gen AI SDK for Python with the Gemini 2.0 Flash model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text from text prompts\n",
        "  - Generate streaming text\n",
        "  - Start multi-turn chats\n",
        "  - Use asynchronous methods\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgernCnR_xlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "c02ed1d1-8d11-4666-ca29-47380bbdb293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m143.4/144.7 kB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services.\n",
        "\n",
        "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymmEN6GSTn-"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    *   Run the cell below to set your project ID.\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    *   Run the cell further below to use your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1933326c939"
      },
      "source": [
        "#### Option 1. Use a Google Cloud Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UCgUOv4nSWhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"hack2skill-project\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aa38ee3158"
      },
      "source": [
        "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
        "\n",
        "Uncomment the following block to use Express Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpIPG_YhSjaw"
      },
      "outputs": [],
      "source": [
        "# API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
        "\n",
        "# if not API_KEY or API_KEY == \"[your-api-key]\":\n",
        "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
        "\n",
        "# client = genai.Client(vertexai=True, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36ce4ac022"
      },
      "source": [
        "Verify which mode you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8338643f335f",
        "outputId": "abbe4ed7-c521-45e5-8faf-b6fa9d955252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vertex AI with project: hack2skill-project in location: us-central1\n"
          ]
        }
      ],
      "source": [
        "if not client.vertexai:\n",
        "    print(f\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.0 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.0 Flash model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "b674793c-d527-4b96-b58d-10596d431e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break down how to approach winning the Encode Club AI Blueprints hackathon, focusing on simplicity, innovation, Filecoin, and Randamu.\n\n**Understanding the Core Objectives**\n\n*   **Filecoin Integration:**  Leverage Filecoin for decentralized data storage.  Think about how your AI solution generates, consumes, or manages data and how Filecoin can make that more secure, censorship-resistant, and scalable.\n*   **Randamu Integration:**  Randamu provides verifiable randomness. This is critical for many AI applications, especially those involving fairness, unpredictability (e.g., in simulations or games), and avoiding bias.\n*   **Simplicity & Innovation:** The solution should be easy to understand and implement.  Avoid over-engineering. Focus on a novel use case or a clever application of existing tools. Aim for impact, not just complexity.\n*   **Solving Complexities/Challenges:** Identify a real problem in the AI space or a challenge associated with using AI in a particular context.  Your solution should directly address this.\n*   **AI Blueprints:** Your submission should be a *blueprint*.  Think of it as a well-documented, reproducible recipe.  Clear instructions, modular design, and good testing are crucial.\n\n**Roadmap to Success**\n\n1.  **Brainstorming & Problem Definition (1-2 Days):**\n\n    *   **Research:** Deep dive into Filecoin, Randamu, and the current state of AI. Look for gaps, inefficiencies, or unmet needs. Explore existing projects on Filecoin and Randamu to identify opportunities.\n    *   **Identify a Problem:**  Focus on a specific, solvable problem that can be tackled with Filecoin, Randamu, and AI.  Consider these categories:\n        *   **Data Provenance & Security:** How can Filecoin and Randamu ensure the integrity and trustworthiness of AI training data?\n        *   **Decentralized AI Training:** Can we train AI models on decentralized datasets stored on Filecoin, using Randamu for unbiased data selection?\n        *   **AI-Powered Data Management on Filecoin:**  Can AI help users discover, organize, and utilize the vast datasets available on Filecoin?\n        *   **Fair & Unbiased AI:**  Can Randamu be used to mitigate bias in AI algorithms and ensure fair outcomes?\n        *   **AI-Driven Content Creation with Verifiable Randomness:**  Can AI generate content (e.g., art, music, stories) where the randomness is verifiably fair and unpredictable using Randamu?\n    *   **Define Success Metrics:** How will you measure the success of your project? (e.g., improved data availability, reduced bias, faster training times, increased security)\n\n2.  **Solution Design & Architecture (2-3 Days):**\n\n    *   **Choose your AI Model/Technique:**  Select an appropriate AI model (e.g., classification, regression, generative model). Keep it simple for the hackathon.  Consider using pre-trained models to save time.\n    *   **Design the Data Flow:**  How will data be ingested, processed, stored on Filecoin, and used by the AI model?\n    *   **Integrate Filecoin:** Design how your application interacts with Filecoin. Will you be storing raw data, processed data, model weights, or metadata on Filecoin?  Use the Filecoin SDK or relevant APIs.\n    *   **Integrate Randamu:** Decide how Randamu's verifiable randomness will be used in your solution. Examples:\n        *   Randomly selecting data for training to prevent bias.\n        *   Introducing randomness in the AI model's parameters to improve exploration.\n        *   Generating unique and unpredictable content.\n    *   **Create an Architecture Diagram:**  Visualize your solution.\n    *   **Prototype:** Build a basic prototype to test your core ideas.\n    *   **Iterate:** Refine your design based on your prototype.\n\n3.  **Implementation & Testing (3-4 Days):**\n\n    *   **Develop Core Components:** Implement the core components of your solution (data ingestion, Filecoin integration, Randamu integration, AI model).\n    *   **Write Unit Tests:** Test each component thoroughly.\n    *   **Write Integration Tests:** Test the interaction between different components.\n    *   **Address Bugs:** Fix any bugs you find.\n\n4.  **Documentation & Presentation (1-2 Days):**\n\n    *   **Write Clear Documentation:**  This is *crucial*. Your documentation should include:\n        *   **Project Overview:**  What problem are you solving?\n        *   **Architecture:**  Explain your architecture diagram.\n        *   **Filecoin Integration:** How are you using Filecoin?\n        *   **Randamu Integration:** How are you using Randamu?\n        *   **Code Walkthrough:** Explain the key parts of your code.\n        *   **Installation Instructions:**  How to set up and run your project.\n        *   **Usage Examples:**  How to use your project.\n        *   **Future Improvements:**  What could be improved in the future?\n    *   **Create a Compelling Presentation:**  Use visuals (diagrams, demos).  Highlight the problem, your solution, the innovation, and the impact. Practice your presentation.\n    *   **Prepare a Demo:**  Showcase your project in action. A video demo is often a good idea.\n\n**Architectural Workflow Diagrams**\n\nHere are example diagrams for two potential project ideas. Adapt these based on your chosen problem.\n\n**Project 1: Decentralized & Unbiased AI Training Data**\n\n*   **Problem:** AI models trained on biased datasets can perpetuate unfair outcomes.\n*   **Solution:** Use Filecoin to store a diverse, decentralized dataset, and use Randamu to randomly sample data for training in a verifiable and unbiased manner.\n\n```mermaid\ngraph LR\n    A[Data Sources (Various)] --> B(Filecoin Storage);\n    B --> C{Data Indexing & Metadata};\n    C --> D(Randamu - Random Data Selector);\n    D --> E[AI Training Pipeline];\n    E --> F(Trained AI Model);\n    F --> G[Evaluation & Monitoring];\n    G --> H{Feedback Loop};\n    H --> D;\n\n    subgraph Data Storage & Selection\n        B; C; D;\n    end\n\n    subgraph AI Training\n        E; F; G; H;\n    end\n\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style D fill:#ccf,stroke:#333,stroke-width:2px\n```\n\n**Explanation:**\n\n1.  **Data Sources:**  Data comes from various sources (e.g., public datasets, community contributions).\n2.  **Filecoin Storage:** The data is stored on Filecoin, ensuring decentralized and secure storage.  Metadata about the data (source, features, etc.) is also stored.\n3.  **Data Indexing & Metadata:**  An indexing service (e.g., ElasticSearch) allows for searching and filtering data based on its metadata.\n4.  **Randamu - Random Data Selector:** Randamu is used to select a random subset of the data for training. The randomness is verifiable, ensuring fairness.\n5.  **AI Training Pipeline:** The selected data is fed into an AI training pipeline.\n6.  **Trained AI Model:** A trained AI model is produced.\n7.  **Evaluation & Monitoring:** The model is evaluated for bias and performance.\n8.  **Feedback Loop:**  Feedback from evaluation is used to refine the data selection process (using Randamu) and improve the model.\n\n**Project 2: AI-Generated Art with Verifiable Randomness**\n\n*   **Problem:**  AI-generated art can sometimes lack originality and unpredictability.\n*   **Solution:** Use Randamu to introduce verifiable randomness into the art generation process, ensuring unique and fair outputs.  Store the generated art (and the Randamu seed used to create it) on Filecoin.\n\n```mermaid\ngraph LR\n    A[User Input (Optional)] --> B(AI Art Generation Model);\n    B --> C(Randamu - Random Seed Generator);\n    C --> B;\n    B --> D[Generated Art];\n    D --> E(Filecoin Storage);\n    E --> F{Metadata (Randamu Seed, etc.)};\n    F --> E;\n    E --> G[NFT Minting (Optional)];\n\n    style E fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#ccf,stroke:#333,stroke-width:2px\n```\n\n**Explanation:**\n\n1.  **User Input (Optional):** The user provides optional input to influence the art generation (e.g., style, keywords).\n2.  **AI Art Generation Model:**  An AI art generation model (e.g., GAN, VAE) is used to create the art.\n3.  **Randamu - Random Seed Generator:**  Randamu generates a verifiable random seed. This seed is used to influence the AI model's output, adding randomness and unpredictability.\n4.  **Generated Art:**  The AI model generates the art based on the user input (if any) and the random seed.\n5.  **Filecoin Storage:** The generated art (image or video) is stored on Filecoin.\n6.  **Metadata:**  Metadata about the art (including the Randamu seed used to create it) is stored along with the art on Filecoin. This allows for verifying the randomness and originality of the art.\n7.  **NFT Minting (Optional):** The art can be minted as an NFT, with the Filecoin CID and the Randamu seed stored in the NFT's metadata.\n\n**Key Considerations for Winning:**\n\n*   **Impact:**  Clearly demonstrate the value of your solution.  How does it solve a real problem?\n*   **Innovation:**  What's unique about your approach?\n*   **Technical Excellence:**  Write clean, well-documented, and tested code.\n*   **Filecoin and Randamu Integration:**  Demonstrate a strong understanding of Filecoin and Randamu and how to effectively integrate them into your project.\n*   **Simplicity:**  Don't overcomplicate things. A simple, well-executed solution is better than a complex, buggy one.\n*   **Presentation:**  A clear and compelling presentation can make all the difference.\n\n**Tools and Technologies**\n\n*   **Programming Languages:** Python (popular for AI) or JavaScript/TypeScript (for web apps)\n*   **AI Frameworks:** TensorFlow, PyTorch\n*   **Filecoin SDK:**  js-ipfs-http-client (for JavaScript), or equivalent libraries in other languages.\n*   **Randamu SDK:**  Refer to Randamu's documentation for their API and SDK.\n*   **Web Frameworks:**  React, Vue, Angular (for building a user interface)\n*   **Smart Contract Languages:** Solidity (if you need smart contracts for your solution)\n\n**Important Tips**\n\n*   **Start Early:** Don't wait until the last minute to start working on your project.\n*   **Collaborate:**  Work with others to brainstorm ideas, share knowledge, and divide tasks.\n*   **Seek Feedback:**  Get feedback from other developers and mentors.\n*   **Stay Focused:**  Don't try to do too much. Focus on a specific problem and solve it well.\n*   **Have Fun!**  Hackathons are a great way to learn new things and meet new people.\n\nBy following this roadmap and focusing on simplicity, innovation, and a strong understanding of Filecoin and Randamu, you'll significantly increase your chances of winning the Encode Club AI Blueprints hackathon! Good luck!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"how to win this hackathon under filecoin and randamu project given in this link- https://www.encode.club/ai-blueprints solving the complexities and challenges in a simple, innovative way.?ensure u provide me with a comprehensive, clear roadmap and architectural workflow diagrammatic representation for each project\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "e212b048-558c-43c0-c6b3-624326d5897d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ", let'"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "s break down the potential of your idea for the Encode Hackathon, and how"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " to strategically position it to maximize your chances of success.\n\n**Overall Assessment:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Good, but needs refinement to be \"mindblowing\" and a strategic hackathon win.**\n\nHere's a breakdown of the pros, cons, and how to level"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " it up:\n\n**Pros:**\n\n*   **Relevant Problem:** You've identified a genuine pain point in the Filecoin ecosystem: discoverability.  File"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "coin's strength (content addressing) can also be a weakness without good metadata.  Hackathon judges will appreciate you targeting a real-world challenge.\n*   **Combines Hot Technologies:** AI and decentralized storage are both buzzwords that"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " attract attention. Marrying them is generally a good strategy.\n*   **Clear Technical Implementation:** You've outlined the key technologies involved, demonstrating you've thought about the practical aspects.\n*   **Filecoin Integration:** Directly"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " leveraging Filecoin, IPFS, and potentially Libp2p shows you're engaging with the platform's core features. This is crucial for a Filecoin-focused hackathon.\n*   **Potential for Impact:** If successful, this could significantly improve the usability of Filecoin, which is a compelling selling"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " point.\n\n**Cons/Areas for Improvement:**\n\n*   **Broad and Potentially Overambitious:**  \"AI-Driven Content Addressable Data Curation and Discovery\" is a very large space.  Trying to do *everything* (NLP, computer vision, ML classification/clustering, *all* metadata extraction"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ") in a short hackathon will likely lead to a mediocre implementation.  Judges prefer a *focused* project that delivers a *polished* experience.\n*   **Missing the \"Wow\" Factor:**  While helpful, automated metadata extraction isn't inherently \"mindblowing\" on its own.  It's *"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "useful*, but needs a layer of innovation to truly stand out.\n*   **Lack of Specificity:** What *kind* of data are you targeting? What *specific* problem are you solving for *whom*?\n*   **Competitive Landscape:**  Metadata extraction tools already exist.  You need to clearly"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " differentiate your approach.\n\n**Strategic Recommendations to Make This a Hackathon Winner:**\n\n1.  **Sharpen the Focus: Define a Niche**\n\n    *   **Choose a Specific Data Type:**  Don't try to be a general-purpose metadata extractor.  Instead, focus on a specific type of content:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        *   **Examples:**  Scientific datasets (papers, experimental results), Creative Commons licensed images/videos, Decentralized social media posts, Legal documents, Music files, etc.\n    *   **Choose a Specific User Group:** Who will benefit most from your tool? Researchers, artists, content creators, lawyers"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "? Understanding your user will help you prioritize features and design the user experience.\n    *   **This narrower focus will allow you to build something *deep* and *impressive* within the hackathon timeframe.**\n\n2.  **Add the \"Wow\" Factor: Innovation Beyond Basic Metadata**\n\n    *   "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Semantic Search/Reasoning:**  Instead of *just* extracting keywords, can you build a system that *understands* the relationships between concepts within the data?  Think beyond keyword search to semantic search.  For example, can it understand that \"Feline\" and \"Cat\" are related?\n    *"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "   **Contextualization:** Can you analyze the data in the context of other data on Filecoin?  For example, if you're looking at a scientific paper, can you find related datasets or discussions?\n    *   **Recommendation Engine:** Based on a user's search history or interests, can you recommend"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " relevant data on Filecoin?\n    *   **Gamification/Social Features:** Can you add elements that encourage users to curate and improve metadata?  Rewards for labeling data or identifying errors.\n\n3.  **Demonstrate Tangible Value: Focus on User Experience (UX)**\n\n    *   **Build a Simple, Beautiful"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " UI:**  A polished user interface will make a huge difference in how the judges perceive your project.  Even if the backend is complex, make the front-end easy to use and understand.\n    *   **Interactive Demo:** Create a demo that allows judges to easily search and explore the curated data.\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*   **Clear Value Proposition:** Explain clearly how your project solves a specific problem for a specific user group, and how it makes Filecoin more useful.\n\n4.  **Strategic Technology Choices & Implementation:**\n\n    *   **Prioritize NLP:** NLP is generally easier to get working in a hackathon timeframe than computer vision ("
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "unless you have very specific experience).\n    *   **Leverage Pre-trained Models:**  Don't try to train your own NLP or computer vision models from scratch.  Use pre-trained models from Hugging Face, TensorFlow Hub, or similar resources.\n    *   **Focus on a Core Feature:**  Instead"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " of trying to implement all the AI features, prioritize one or two core features that you can implement well.\n    *   **Efficient Filecoin Integration:**\n        *   Think about how you will handle large files efficiently.\n        *   Consider using Filecoin's retrieval market to get data faster.\n        *"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "   Make sure you are correctly storing and retrieving data using CIDs.\n\n**Example Scenario: Leveling Up Your Idea**\n\nInstead of: \"AI-Driven Content Addressable Data Curation and Discovery\"\n\nTry this:\n\n**\"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets\"**\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*   **Problem:** Scientists struggle to find relevant datasets stored on Filecoin because of a lack of semantic understanding and contextualization.\n*   **Solution:**  A system that automatically extracts metadata (keywords, concepts, relationships) from scientific papers and datasets stored on Filecoin using NLP. It then creates a graph database"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " that allows users to explore the data in a semantic way, finding related datasets, papers, and researchers.\n*   **Innovation:**  Semantic search and contextualization of scientific data on Filecoin, enabling researchers to discover new insights and collaborations.\n*   **Tech:** Filecoin, IPFS, Libp2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "p, NLP (using a pre-trained scientific language model), Graph Database (e.g., Neo4j).\n*   **UI:**  A web interface that allows users to search for datasets, explore the data graph, and find related resources.\n\n**In Summary:**\n\nYour initial idea has potential,"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " but it needs to be sharpened and focused. By defining a specific niche, adding a \"wow\" factor, focusing on user experience, and making strategic technology choices, you can significantly increase your chances of success at the Encode Hackathon.  Good luck! Remember to test your implementation for specific use cases.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me wisely and strategically, does building on this would be unique, innovative and mindblowing for Encode hackathon or not? Here is the Idea : AI-Driven Content Addressable Data Curation and Discovery. Problem: Filecoin uses content addressing, but it can be difficult to find specific data if you don't know the CID (Content Identifier). Metadata is essential for discovery. Solution: An AI-powered system that automatically extracts metadata from files stored on Filecoin and creates a searchable index. This could use natural language processing (NLP) and computer vision to understand the content of the files. Innovation: Makes data stored on Filecoin more discoverable and accessible. Filecoin Technologies Used: Filecoin Content Addressing (CID), IPFS (InterPlanetary File System), Libp2p. AI Technologies Used: Natural Language Processing (NLP), Computer Vision, Machine Learning (for classification and clustering), Metadata Extraction.\",\n",
        "):\n",
        "    display(Markdown(chunk.text))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "1723e898-dd4c-4dc3-99e6-f68028c1bee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break down how to make this \"Filecoin Data Graph\" MVP a winning Encode hackathon project. We'll focus on:\n\n1.  **Core Functionality (MVP):** Identify the absolute minimum features to demonstrate the value proposition.\n2.  **Feasibility:** Ensure the scope is achievable within the hackathon timeframe.\n3.  **Impact & Innovation:** Highlight the project's potential and novel aspects.\n4.  **Presentation:** Consider how to effectively showcase the project to judges.\n\nHere's a suggested refined plan, prioritizing speed and impact:\n\n**I. MVP Core Functionality (Focus on a *Single* Scientific Domain):**\n\nInstead of trying to handle *all* scientific data, let's focus on a specific, well-defined area.  This significantly reduces the complexity of the NLP and dataset selection.  A good choice might be:\n\n*   **Option 1: Protein Structures & Bioinformatics:**  Tons of readily available datasets (Protein Data Bank - PDB), pre-trained models often available, high demand in research.  Focus on searching for proteins with similar functions or structures.\n*   **Option 2: Climate Change Data:** Datasets related to temperature, CO2 levels, weather patterns.  Easily demonstrable impact, clear keywords.\n*   **Option 3: COVID-19 Research Data:** Datasets related to viral sequences, treatments, patient data.  Timely and relevant.\n\n*Choosing one domain allows for deeper analysis and better demo results.*\n\n**MVP Features:**\n\n1.  **Data Ingestion (Automated or Semi-Automated):**\n    *   **Automated (Ideal):** Script to fetch metadata and/or data from a specific Filecoin CID (or set of CIDs) relevant to your chosen domain. If truly automated ingestion is beyond the timeframe, use a limited set of pre-defined CIDs/datasets for the chosen domain.  Emphasize this as \"proof of concept\" for a future automated system.\n    *   **Semi-Automated (Acceptable):** Manually upload data and metadata for a *small* set of relevant datasets (3-5 datasets).  Focus on a well-documented upload process that could *easily* be automated.\n    *   **Key Metadata:** Each dataset needs:\n        *   Filecoin CID\n        *   Title/Description\n        *   Keywords/Concepts (Extracted via NLP - see below)\n        *   Links to related papers (if available)\n\n2.  **NLP Pipeline (Focused & Pragmatic):**\n    *   **Keyword Extraction:**  Use a pre-trained scientific language model (e.g., SciBERT, BioBERT, depending on your domain). The goal is to extract relevant keywords and concepts *automatically* from dataset descriptions and associated papers.\n    *   **Simplify:** Don't try to build a complex model from scratch. Fine-tune an existing model if you have time, but focus on using it *effectively* for keyword extraction.\n    *   **Focus on Accuracy:**  Prioritize getting *accurate* keywords over trying to extract every possible detail. A few well-chosen keywords are better than many inaccurate ones.\n\n3.  **Graph Database:**\n    *   **Neo4j (Recommended):** Relatively easy to set up and use. Great for visualizing relationships.\n    *   **Simple Graph Schema:**  Nodes represent:\n        *   Datasets (Filecoin CIDs)\n        *   Papers\n        *   Researchers (if you can extract author info)\n        *   Keywords/Concepts\n    *   Edges represent relationships (e.g., \"contains,\" \"is related to,\" \"authored by\").\n    *   *Crucially*, build the graph *programmatically*.  Show you can add nodes and edges dynamically.\n\n4.  **Search & Exploration UI:**\n    *   **Simple Search Bar:** Allow users to search for datasets by keyword.\n    *   **Graph Visualization:**  Show the connections between datasets, papers, and concepts related to the search term.  Neo4j Bloom is a good option for easy graph visualization.  Limit to showing *direct* connections initially.\n    *   **Dataset Details:**  Clicking on a dataset node should display the relevant metadata (Filecoin CID, description, keywords, links).\n    *   **Minimalist Design:**  Focus on functionality over aesthetics.  Use a pre-built UI framework (e.g., React, Vue.js) to save time.\n\n**II. Tech Stack (Prioritized):**\n\n*   **Filecoin/IPFS:**  Central to the project. Demonstrate storing *metadata* on Filecoin (the actual datasets likely already are).  Use the CID as the primary key for your datasets.\n*   **Libp2p:**  *Consider removing for the MVP.*  It adds significant complexity.  You can mention it as a future direction (decentralized data indexing), but don't try to implement it within the hackathon timeframe.\n*   **Python:**  For NLP (SciBERT, etc.) and data processing.\n*   **Neo4j:** Graph database.\n*   **Frontend Framework (React/Vue.js):** For the UI.\n\n**III. Innovation & Impact (Hackathon Advantage):**\n\n*   **Focus on *Semantic* Search:**  Emphasize how your project goes beyond simple keyword searching.  Highlight the graph database's ability to discover *relationships* and connections that would be missed by traditional search methods.  The NLP pipeline is critical to this.\n*   **Filecoin Integration:**  Clearly articulate the benefit of indexing *Filecoin-stored data*.  Talk about the potential to unlock the vast amounts of scientific data already on Filecoin and make it more accessible to researchers.\n*   **Collaboration:**  Show how the system could facilitate collaboration by connecting researchers working on related topics.\n*   **Future Potential:**  Mention potential extensions, such as:\n    *   Automated dataset ingestion from multiple sources.\n    *   More sophisticated NLP techniques (e.g., relationship extraction, topic modeling).\n    *   Decentralized indexing using Libp2p.\n    *   Integration with other scientific tools and platforms.\n\n**IV. Test Cases (Essential for Demonstrating Reliability):**\n\n*   **Data Ingestion Test:** Verify that the system can correctly ingest data and metadata from Filecoin.  Check for data integrity.\n*   **NLP Test:** Evaluate the accuracy of keyword extraction.  Manually review the extracted keywords for a sample of datasets.\n*   **Graph Database Test:**  Confirm that the graph database is correctly storing and retrieving data.  Test the relationships between nodes.\n*   **Search Test:**  Ensure that the search functionality returns relevant results.  Test with different keywords and combinations.\n*   **UI Test:**  Verify that the UI is responsive and user-friendly.\n\n**V.  Presentation (Make it Compelling):**\n\n*   **Clear Problem Statement:**  Start with the pain point: scientists struggling to find relevant data on Filecoin.\n*   **Demonstrate the MVP:**  Show the search functionality, the graph visualization, and the dataset details.  Focus on *one or two compelling use cases* within your chosen domain.\n*   **Highlight the Innovation:**  Explain how your semantic search approach is better than traditional methods.\n*   **Show the Code (Briefly):**  Demonstrate the NLP pipeline and the graph database integration.\n*   **Future Vision:**  Paint a picture of the project's potential.\n*   **Prepare for Questions:**  Anticipate questions about scalability, data quality, and future development.\n\n**VI.  Timeline (Crucial for Hackathons):**\n\n*   **Day 1:**\n    *   Choose a specific scientific domain.\n    *   Set up the basic tech stack (Filecoin, Neo4j, Python environment, frontend framework).\n    *   Implement data ingestion (manual or semi-automated) for a few datasets.\n*   **Day 2:**\n    *   Implement the NLP pipeline (keyword extraction).\n    *   Build the graph database schema.\n    *   Populate the graph database with data from the ingested datasets.\n*   **Day 3:**\n    *   Develop the search functionality.\n    *   Create the UI for graph visualization and dataset details.\n    *   Implement test cases.\n*   **Day 4:**\n    *   Refine the UI.\n    *   Write documentation.\n    *   Practice the presentation.\n\n**Key Takeaways for Winning:**\n\n*   **Scope:** Focus on a *narrow* scope and do it *extremely well*.\n*   **Demonstration:** Make sure the core functionality is working and easy to demonstrate.\n*   **Impact:**  Clearly articulate the value proposition and potential impact.\n*   **Filecoin Integration:**  Highlight how your project leverages Filecoin's unique capabilities.\n*   **Innovation:**  Emphasize the novel aspects of your approach.\n*   **Presentation:**  Tell a compelling story and showcase the project's strengths.\n\nBy focusing on a well-defined MVP, implementing essential features, and crafting a compelling presentation, you'll significantly increase your chances of winning the Encode hackathon.  Good luck!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"lets work on enlivening an optimal, working MVP aspect our idea for winning the Encode hackathon strategically, here is the outlined idea: Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets. Problem: Scientists struggle to find relevant datasets stored on Filecoin because of a lack of semantic understanding and contextualization. Solution: A system that automatically extracts metadata (keywords, concepts, relationships) from scientific papers and datasets stored on Filecoin using NLP. It then creates a graph database that allows users to explore the data in a semantic way, finding related datasets, papers, and researchers. Innovation: Semantic search and contextualization of scientific data on Filecoin, enabling researchers to discover new insights and collaborations. Tech: Filecoin, IPFS, Libp2p, NLP (using a pre-trained scientific language model), Graph Database (e.g., Neo4j). Test: Implementing test cases at necessary steps for verifying seamless workflow. UI: A modern, web interface that allows users to search for datasets, explore the data graph, and find related resources.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "fd124d4e-0021-4508-d0cc-03842c17fc75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's architect and begin coding our MVP for a Protein Structures & Bioinformatics platform. This will be an iterative process, focusing first on the core features and a clean, intuitive user experience.\n\n**MVP Goal:**  A platform to search for protein structures, visualize them, and perform basic sequence analysis, all in a user-friendly interface.\n\n**Technologies (Revisited and Specified):**\n\n*   **Frontend:**\n    *   **React:** For building a dynamic and responsive user interface.\n    *   **Typescript:** For type safety and maintainability.\n    *   **Redux/Context API:** For state management (choose one based on project complexity, starting with Context API for simplicity).\n    *   **3D Visualization Library (Choose one):**\n        *   **NGL (Preferred):**  Powerful, performant, and specifically designed for molecular visualization. Supports many file formats (PDB, etc.).\n        *   **Three.js:**  More general-purpose, but requires more manual setup for molecular rendering.\n    *   **UI Library (Choose one):**\n        *   **Material UI (MUI):** Comprehensive, customizable, and well-documented.  Good for a consistent look and feel.\n        *   **Ant Design (AntD):** Another popular choice with a modern aesthetic.\n        *   **Chakra UI:**  Simple, composable components.\n*   **Backend:**\n    *   **Python:** Versatile and widely used in bioinformatics.\n    *   **Flask/FastAPI:** Lightweight web frameworks for creating APIs.  FastAPI offers automatic data validation and documentation.\n    *   **BioPython:** Essential library for sequence analysis, structure parsing, and bioinformatics algorithms.\n    *   **Databases (Choose one):**\n        *   **PostgreSQL:** Relational database, reliable and scalable. Good for structured data and complex queries.\n        *   **MongoDB:** NoSQL database, flexible schema, good for semi-structured data.\n    *   **Celery (with Redis/RabbitMQ):** For asynchronous task processing (e.g., computationally intensive sequence alignments).\n*   **Data Sources:**\n    *   **RCSB Protein Data Bank (PDB):** The primary source for protein structure data.  We will use their API.\n    *   **UniProt:**  For protein sequence information.\n    *   **(Optional) NCBI BLAST:**  For sequence similarity searches.\n\n**Architecture:**\n\n```\n+---------------------+      +---------------------+      +---------------------+\n|      Frontend       |      |       Backend        |      |      Data Sources    |\n|  (React/Typescript) |      |    (Python/Flask)    |      | (RCSB PDB, UniProt) |\n+---------------------+      +---------------------+      +---------------------+\n         |                       |                       |\n         |  API Requests         |                       |\n         +---------------------> |                       |\n         |                       |  Data Retrieval     |\n         |                       +---------------------> |\n         |                       |                       |\n         |  Data Rendering       |  Data Processing      |\n         <--------------------- +--------------------- |\n```\n\n**MVP Features:**\n\n1.  **Protein Search:**\n    *   Search by PDB ID (e.g., \"1AKE\").\n    *   Search by protein name or keyword (e.g., \"hemoglobin\").\n2.  **Protein Structure Visualization:**\n    *   Display the 3D structure using NGL.\n    *   Allow basic manipulations (rotation, zoom, translation).\n    *   Highlight specific residues/chains.\n3.  **Sequence Retrieval and Display:**\n    *   Display the protein sequence.\n    *   Basic sequence information (length, etc.).\n4.  **(Basic) Sequence Analysis:**\n    *   Calculate molecular weight.\n    *   Calculate isoelectric point (pI).\n\n**Phase 1: Setting up the Project (Frontend)**\n\n1.  **Create React App with Typescript:**\n\n```bash\nnpx create-react-app protein-viewer --template typescript\ncd protein-viewer\n```\n\n2.  **Install Dependencies:**\n\n```bash\nnpm install @mui/material @emotion/react @emotion/styled @mui/icons-material  ngl react-redux @reduxjs/toolkit\n# OR using yarn\nyarn add @mui/material @emotion/react @emotion/styled @mui/icons-material  ngl react-redux @reduxjs/toolkit\n```\n\n3.  **Project Structure (Frontend):**\n\n```\nprotein-viewer/\n├── src/\n│   ├── components/     # Reusable UI components\n│   │   ├── SearchBar.tsx\n│   │   ├── StructureViewer.tsx\n│   │   ├── SequenceViewer.tsx\n│   │   └── ...\n│   ├── pages/        # Different routes/pages\n│   │   └── ProteinDetails.tsx\n│   ├── services/     # API interaction\n│   │   └── proteinService.ts\n│   ├── store/        # Redux Store or Context\n│   │    └──  store.ts (if using Redux)\n│   │    └──  ProteinContext.tsx (if using Context)\n│   ├── App.tsx\n│   └── index.tsx\n├── public/\n├── package.json\n└── ...\n```\n\n4.  **`App.tsx` (Initial Setup):**\n\n```typescript\nimport React from 'react';\nimport ProteinDetails from './pages/ProteinDetails';\nimport { ThemeProvider, createTheme } from '@mui/material/styles';\nimport CssBaseline from '@mui/material/CssBaseline';\nconst darkTheme = createTheme({\n  palette: {\n    mode: 'dark',\n  },\n});\n\nfunction App() {\n  return (\n      <ThemeProvider theme={darkTheme}>\n          <CssBaseline/>\n          <ProteinDetails />\n      </ThemeProvider>\n  );\n}\n\nexport default App;\n```\n\n5.  **`pages/ProteinDetails.tsx` (Basic Layout):**\n\n```typescript\nimport React, { useState } from 'react';\nimport SearchBar from '../components/SearchBar';\nimport StructureViewer from '../components/StructureViewer';\nimport SequenceViewer from '../components/SequenceViewer';\nimport { Container, Typography } from '@mui/material';\n\nconst ProteinDetails: React.FC = () => {\n  const [pdbId, setPdbId] = useState<string | null>(null);\n\n  const handleSearch = (id: string) => {\n    setPdbId(id);\n  };\n\n  return (\n    <Container maxWidth=\"lg\">\n      <Typography variant=\"h4\" component=\"h1\" gutterBottom>\n        Protein Viewer\n      </Typography>\n      <SearchBar onSearch={handleSearch} />\n      {pdbId && (\n        <>\n          <StructureViewer pdbId={pdbId} />\n          <SequenceViewer pdbId={pdbId} />\n        </>\n      )}\n    </Container>\n  );\n};\n\nexport default ProteinDetails;\n```\n\n6.  **`components/SearchBar.tsx`:**\n\n```typescript\nimport React, { useState } from 'react';\nimport { TextField, Button, Stack } from '@mui/material';\n\ninterface SearchBarProps {\n  onSearch: (id: string) => void;\n}\n\nconst SearchBar: React.FC<SearchBarProps> = ({ onSearch }) => {\n  const [searchTerm, setSearchTerm] = useState('');\n\n  const handleChange = (event: React.ChangeEvent<HTMLInputElement>) => {\n    setSearchTerm(event.target.value);\n  };\n\n  const handleSubmit = () => {\n    onSearch(searchTerm);\n  };\n\n  return (\n      <Stack direction=\"row\" spacing={2}>\n          <TextField\n              label=\"Enter PDB ID\"\n              variant=\"outlined\"\n              value={searchTerm}\n              onChange={handleChange}\n          />\n          <Button variant=\"contained\" color=\"primary\" onClick={handleSubmit}>\n              Search\n          </Button>\n      </Stack>\n  );\n};\n\nexport default SearchBar;\n```\n\n**Phase 2: Setting up the Project (Backend)**\n\n1.  **Create a Backend Directory:** Create a new directory (e.g., `protein-api`) separate from your frontend.\n\n2.  **Initialize a Python Project:**\n\n```bash\nmkdir protein-api\ncd protein-api\npython3 -m venv venv  # Create a virtual environment\nsource venv/bin/activate # Activate the virtual environment (Linux/macOS)\n# or\n# venv\\Scripts\\activate  (Windows)\npip install flask flask-cors biopython requests\n```\n\n3.  **Project Structure (Backend):**\n\n```\nprotein-api/\n├── app.py         # Main Flask application\n├── utils.py       # Utility functions (sequence analysis, etc.)\n├── models.py      # (If using an ORM like SQLAlchemy) Database models\n└── venv/          # Virtual environment\n```\n\n4.  **`app.py` (Basic Flask API):**\n\n```python\nfrom flask import Flask, jsonify\nfrom flask_cors import CORS\nimport requests\nfrom Bio import PDB\nfrom Bio.SeqUtils import molecular_weight\n\napp = Flask(__name__)\nCORS(app) # Enable CORS for local development\n\n@app.route('/protein/<pdb_id>')\ndef get_protein_data(pdb_id):\n    try:\n        # Fetch PDB file from RCSB\n        pdb_url = f'https://files.rcsb.org/download/{pdb_id.upper()}.pdb'\n        pdb_response = requests.get(pdb_url)\n\n        if pdb_response.status_code != 200:\n            return jsonify({'error': f'PDB ID {pdb_id} not found'}), 404\n\n        pdb_data = pdb_response.text\n\n        # Basic sequence analysis (example)\n        parser = PDB.PDBParser(QUIET=True)\n        structure = parser.feed_string(pdb_data)\n        sequence = ''\n        for model in structure:\n            for chain in model:\n                for residue in chain:\n                    if residue.has_id('CA'): # Alpha carbon\n                        try:\n                            sequence += PDB.Polypeptide.one_to_three(residue.resname)\n                        except KeyError:\n                            pass # Ignore non-standard residues\n\n        if sequence:\n             mw = molecular_weight(sequence, seq_type='protein')\n        else:\n             mw = None\n\n\n        return jsonify({\n            'pdb_data': pdb_data, # Return the full PDB data\n            'molecular_weight': mw\n        })\n\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(debug=True)  # Enable debug mode for development\n```\n\n**Phase 3: Connecting Frontend and Backend**\n\n1.  **`services/proteinService.ts` (Frontend):**\n\n```typescript\nimport axios from 'axios';\n\nconst API_URL = 'http://localhost:5000'; // Adjust if your backend runs on a different port\n\nexport const getProteinData = async (pdbId: string) => {\n  try {\n    const response = await axios.get(`${API_URL}/protein/${pdbId}`);\n    return response.data;\n  } catch (error: any) {\n    console.error(\"Error fetching protein data:\", error);\n    throw error; // Re-throw for handling in the component\n  }\n};\n```\n\n2.  **Update `StructureViewer.tsx` (Frontend):**\n\n```typescript\nimport React, { useEffect, useRef } from 'react';\nimport { getProteinData } from '../services/proteinService';\nimport { Box } from '@mui/material';\nimport * as NGL from 'ngl';\n\ninterface StructureViewerProps {\n  pdbId: string;\n}\n\nconst StructureViewer: React.FC<StructureViewerProps> = ({ pdbId }) => {\n  const stageRef = useRef<HTMLDivElement>(null);\n  const stageInstance = useRef<NGL.Stage | null>(null);\n\n  useEffect(() => {\n    const initializeViewer = async () => {\n      if (!pdbId) return;\n\n      try {\n        const data = await getProteinData(pdbId);\n        if (data && data.pdb_data) {\n          if (!stageRef.current) {\n            return; // Component unmounted\n          }\n          if (!stageInstance.current) {\n             stageInstance.current = new NGL.Stage(stageRef.current, { backgroundColor: \"black\" });\n          } else {\n            stageInstance.current.removeAllComponents();  // Clear existing components\n          }\n          stageInstance.current.loadFile(data.pdb_data, { ext: 'pdb', defaultRepresentation: true });\n          stageInstance.current.autoView();\n\n\n        } else {\n          console.error(\"Invalid protein data received\");\n        }\n      } catch (error) {\n        console.error(\"Error loading protein structure:\", error);\n      }\n    };\n\n    initializeViewer();\n\n    return () => {\n        if (stageInstance.current) {\n            stageInstance.current.dispose();  // Dispose of NGL stage\n            stageInstance.current = null;\n        }\n    };\n  }, [pdbId]);\n\n  return (\n    <Box ref={stageRef} sx={{ width: '100%', height: '500px' }} />\n  );\n};\n\nexport default StructureViewer;\n```\n\n3.  **Update `SequenceViewer.tsx` (Frontend):**\n\n```typescript\nimport React, { useState, useEffect } from 'react';\nimport { getProteinData } from '../services/proteinService';\nimport { Typography, Box } from '@mui/material';\n\ninterface SequenceViewerProps {\n  pdbId: string;\n}\n\nconst SequenceViewer: React.FC<SequenceViewerProps> = ({ pdbId }) => {\n  const [sequence, setSequence] = useState<string>('');\n  const [molecularWeight, setMolecularWeight] = useState<number | null>(null);\n\n  useEffect(() => {\n    const fetchSequence = async () => {\n      try {\n        const data = await getProteinData(pdbId);\n        // Extract the sequence from the PDB data or fetch it from UniProt (more reliable)\n        if (data) {\n             setMolecularWeight(data.molecular_weight);\n             //Extract chain from PDB and parse\n             let seq = '';\n             const lines = data.pdb_data.split('\\n');\n            for (const line of lines) {\n                if (line.startsWith('SEQRES')) {\n                    const parts = line.trim().split(/\\s+/);\n                    // Starting from index 4, extract sequence\n                    for (let i = 4; i < parts.length; i++) {\n                        seq += parts[i];\n                    }\n                }\n            }\n             setSequence(seq);\n\n        }\n      } catch (error) {\n        console.error(\"Error fetching sequence:\", error);\n        setSequence('Error fetching sequence');\n      }\n    };\n\n    fetchSequence();\n  }, [pdbId]);\n\n  return (\n    <Box mt={2}>\n      <Typography variant=\"h6\">Sequence:</Typography>\n      <Typography variant=\"body1\">{sequence}</Typography>\n        {molecularWeight !== null && (\n            <Typography variant=\"body2\">Molecular Weight: {molecularWeight.toFixed(2)} Da</Typography>\n        )}\n\n    </Box>\n  );\n};\n\nexport default SequenceViewer;\n```\n\n**Running the Application:**\n\n1.  **Start the Backend:**\n\n```bash\ncd protein-api\nsource venv/bin/activate\npython app.py\n```\n\n2.  **Start the Frontend:**\n\n```bash\ncd protein-viewer\nnpm start\n```\n\n**Explanation:**\n\n*   **Frontend (React):**\n    *   The `ProteinDetails` component manages the overall layout and state (the `pdbId`).\n    *   `SearchBar` allows users to enter a PDB ID.\n    *   `StructureViewer` uses NGL to display the 3D structure fetched from the backend.\n    *   `SequenceViewer` fetches the sequence from the backend (currently just extracting the SEQRES records in the PDB) and displays it.  It also displays the calculated molecular weight.\n    *   `proteinService.ts` handles the API call to the backend.\n*   **Backend (Flask):**\n    *   The `/protein/<pdb_id>` endpoint fetches the PDB file from the RCSB PDB.\n    *   It does a *very basic* sequence analysis using BioPython.  A more robust implementation would parse the PDB file properly or fetch the sequence from UniProt.\n    *   The API returns the PDB data and the calculated molecular weight in JSON format.\n\n**Next Steps (Iterative Development):**\n\n1.  **Error Handling:** Implement more robust error handling in both the frontend and backend.\n2.  **Loading Indicators:** Add loading spinners/progress bars while data is being fetched.\n3.  **Data Validation:** Use FastAPI for the backend for automatic data validation and documentation.\n4.  **Styling:** Improve the visual appearance with MUI or another UI library.\n5.  **Sequence Alignment:** Implement a more advanced sequence alignment feature (e.g., using NCBI BLAST) using Celery for asynchronous task processing.  This is a more computationally intensive feature.\n6.  **Database Integration:** Store protein data (metadata, sequences) in a database (PostgreSQL or MongoDB) for faster retrieval and more complex queries.\n7.  **Sequence Retrieval from UniProt:**  Replace the simplistic PDB sequence parsing with retrieval from the UniProt API for more accurate sequence information.\n8.  **User Authentication:** Add user accounts for saving favorite structures, analyses, etc.\n\n**Important Considerations:**\n\n*   **Performance:**  NGL is generally very performant, but large structures can still be slow to load. Consider optimization techniques like caching.\n*   **Scalability:**  For a production environment, you'll need to consider scalability for both the frontend and backend (e.g., using a load balancer, multiple backend instances, a CDN for static assets).\n*   **Security:**  Be mindful of security best practices, especially when handling user data.\n\nThis is a foundational MVP.  We can delve deeper into specific aspects or features as you guide the development.  Let me know what you'd like to focus on next!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"lets code the above MVP revolving around this domain: Protein Structures & Bioinformatics in an intuitive, innovative way, using relevant technologies as mentioned above. lets go!\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "f653ab9f-4baa-4a11-9303-17d9d1674644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## What Does It Say?\n\nThe visible fades, a crimson, dying breath,\nInto the cosmic murk, the void of death.\nBut death is just a door, a whisper soft,\nBeyond the reach of light, forever aloft.\n\nPast galaxies like dust motes, scattered far,\nBeyond the quasar's roar, the dying star,\nThe cosmic web, a phantom, thinning thread,\nLies something more profound, a thought unsaid.\n\nThe edges blur, no canvas holds this scene,\nNo telescope can pierce what might have been,\nOr might be still, in realms beyond our ken,\nWhere laws of physics bend, and break again.\n\nWhat whispers there, in tones we cannot hear?\nWhat textures writhe, dispelling every fear,\nOr birthing them anew, in forms unknown?\nA seed of chaos, solitarily sown.\n\nIs it a wall, a barrier of might?\nOr endless, fractal depths, consuming light?\nA mirror darkly gleaming, back at us,\nReflecting ignorance, a silent curse?\n\nThe equations falter, logic starts to crack,\nBefore the vast unknown, there is no track.\nJust emptiness, a silence so profound,\nA question echoing, without a sound.\n\nSo what does it say, beyond the farthest gleam?\nA promise of forever, or a broken dream?\nPerhaps it simply *is*, and nothing more,\nThe boundless, silent keeper, at the universe's core.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a poem that resonates the abysmal and limitless boundary beyond the observable universe with title-What does it say?\");\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "outputId": "c3853e3b-978a-434b-eb60-4f61370f86ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a breakdown of how Filecoin, its API, NLP pipelines, and Neo4j can be leveraged for a protein structure and bioinformatics hackathon project focused on building a \"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets,\" presented in bullet points:\n\n**I. Filecoin & Filecoin API: Decentralized Storage & Data Integrity**\n\n*   **Decentralized Storage of Protein Data:**\n    *   **Problem:**  Centralized repositories for protein structures (e.g., PDB) are vulnerable to single points of failure, censorship, and potential data loss.\n    *   **Solution:** Store protein structure files (PDB, mmCIF), sequence data (FASTA), experimental data (e.g., X-ray diffraction data), and related metadata on Filecoin.\n    *   **Benefits:**  Increased data availability, resilience, and immutability.  Data is verifiable and tamper-proof.\n*   **Data Provenance & Auditability:**\n    *   **Leverage:** Filecoin's storage proofs (Proof-of-Replication, Proof-of-Spacetime) to guarantee data integrity and availability over time.\n    *   **Application:**  Track the origin and modifications of protein structure data.  This is crucial for scientific reproducibility.  Know exactly when a file was stored, by whom, and that it hasn't been altered.\n*   **Economic Incentives for Data Storage:**\n    *   **Benefit:**  Filecoin's economic model incentivizes storage providers to maintain and distribute data.  This can lead to more robust and long-term storage solutions for scientific datasets.\n*   **Filecoin API Integration:**\n    *   **Upload/Download:** Use the Filecoin API (through libraries like `js-ipfs-api` or `go-ipfs-api`) to programmatically upload protein data to Filecoin and retrieve it when needed.\n    *   **Data Retrieval:**  Implement efficient data retrieval mechanisms using Content Identifiers (CIDs) to quickly access specific protein structures.\n    *   **Automated Storage:**  Develop scripts to automatically archive new protein structures from existing databases (e.g., PDB) onto Filecoin.\n*   **Data Curation & Versioning:**\n    *   **Implement:**  Use IPFS's immutable nature to track versions of protein structures.  Each modification results in a new CID, preserving the history of the data.\n    *   **Benefit:**  Researchers can easily access previous versions of a protein structure and understand how it has evolved.\n\n**II. NLP Pipeline: Extracting Knowledge from Scientific Literature**\n\n*   **Purpose:**  Automate the extraction of relevant information about protein structures from scientific publications.\n*   **Components:**\n    *   **Text Extraction:**  Use libraries like `PyPDF2` or `Grobid` to extract text from research papers related to protein structures.\n    *   **Named Entity Recognition (NER):**  Identify and classify key entities in the text, such as:\n        *   Protein names\n        *   Gene names\n        *   Ligands\n        *   Mutations\n        *   Diseases\n        *   Experimental techniques (e.g., X-ray crystallography, NMR)\n    *   **Relation Extraction:**  Identify relationships between the entities identified in the NER step.  Examples:\n        *   \"Protein A interacts with Protein B\"\n        *   \"Protein C is associated with Disease D\"\n        *   \"Mutation E affects the stability of Protein F\"\n    *   **Sentiment Analysis (Optional):**  Determine the sentiment (positive, negative, neutral) expressed towards a particular protein or research finding.\n    *   **Tools & Libraries:**\n        *   spaCy\n        *   NLTK\n        *   Transformers (e.g., BERT, BioBERT)\n        *   SciSpacy\n*   **Data Preprocessing:**\n    *   **Cleaning:** Remove noise, special characters, and irrelevant information from the extracted text.\n    *   **Tokenization:** Break down the text into individual words or tokens.\n    *   **Stop Word Removal:** Remove common words (e.g., \"the,\" \"a,\" \"is\") that don't carry significant meaning.\n    *   **Stemming/Lemmatization:** Reduce words to their root form (e.g., \"running\" -> \"run\").\n*   **Output:**  Structured data representing the extracted information (e.g., JSON, CSV).  This data will be used to populate the Neo4j graph database.\n\n**III. Neo4j: Building a Knowledge Graph**\n\n*   **Purpose:**  Create a graph database that represents the relationships between protein structures, genes, diseases, publications, and other relevant entities.\n*   **Nodes:** Represent entities:\n    *   Protein Structures (linked to Filecoin CIDs)\n    *   Proteins (genes)\n    *   Diseases\n    *   Ligands\n    *   Mutations\n    *   Publications (linked to DOIs or Filecoin CIDs if the full text is stored)\n    *   Experimental Techniques\n*   **Relationships:**  Represent connections between entities:\n    *   `INTERACTS_WITH` (Protein-Protein interactions)\n    *   `ASSOCIATED_WITH` (Protein-Disease associations)\n    *   `BINDS_TO` (Protein-Ligand binding)\n    *   `AFFECTS` (Mutation-Protein effect)\n    *   `STRUCTURE_OF` (Protein Structure - Protein)\n    *   `DESCRIBED_IN` (Protein Structure - Publication)\n*   **Neo4j Cypher Query Language:**\n    *   Use Cypher to query the graph database and discover complex relationships.\n    *   Examples:\n        *   \"Find all proteins that interact with Protein X and are associated with Disease Y.\"\n        *   \"Find all publications that describe the structure of Protein Z and mention Ligand A.\"\n        *   \"Find all mutations that affect the stability of proteins involved in Pathway P.\"\n*   **Graph Algorithms:**\n    *   **Centrality Measures:** Identify the most important proteins or genes in the network.\n    *   **Community Detection:**  Discover groups of proteins that are functionally related.\n    *   **Pathfinding:**  Find the shortest path between two proteins in the network, representing a potential interaction pathway.\n*   **Integration with Filecoin:**\n    *   Store Filecoin CIDs as properties of protein structure nodes in Neo4j.\n    *   When a user queries for a protein structure, the application can retrieve the CID from Neo4j and use the Filecoin API to download the corresponding file.\n\n**IV. Putting it All Together: The Filecoin Data Graph**\n\n1.  **Data Acquisition:**  Gather protein structure data from sources like the PDB, scientific publications, and other bioinformatics databases.\n2.  **Filecoin Storage:**  Store the protein structure files and related metadata on Filecoin, obtaining CIDs for each file.\n3.  **NLP Pipeline:**  Process scientific publications using the NLP pipeline to extract entities and relationships related to protein structures.\n4.  **Neo4j Graph Construction:**  Create nodes and relationships in the Neo4j graph database, linking protein structures (via CIDs) to other relevant entities.\n5.  **User Interface:**  Develop a user interface that allows researchers to:\n    *   Search for protein structures based on various criteria (e.g., protein name, disease association, ligand binding).\n    *   Visualize the relationships between proteins and other entities in the Neo4j graph.\n    *   Download protein structure files directly from Filecoin.\n    *   Explore the provenance and history of protein structures stored on Filecoin.\n6.  **AI-Powered Discovery:**\n    *   Use machine learning models (e.g., graph neural networks) to predict protein-protein interactions, disease associations, or other relevant relationships based on the data in the Neo4j graph.\n    *   Recommend relevant publications or datasets to users based on their search queries and browsing history.\n\n**V. Hackathon Project Considerations**\n\n*   **Focus:**  Choose a specific aspect of the protein structure and bioinformatics domain to focus on (e.g., drug discovery, protein engineering, disease modeling).\n*   **Data:**  Select a relevant dataset to work with (e.g., a subset of the PDB, a collection of scientific publications on a specific disease).\n*   **Scope:**  Define a realistic scope for the project given the time constraints of the hackathon.  It's better to have a well-implemented, focused project than an overly ambitious one that is incomplete.\n*   **Team Skills:**  Leverage the skills of your team members in areas such as data science, software development, and bioinformatics.\n*   **Filecoin Integration:**  Make sure to demonstrate the use of Filecoin for storing and retrieving protein structure data. This is a key aspect of the hackathon challenge.\n*   **Impact:**  Consider the potential impact of your project on the scientific community.  How can your solution help researchers to better understand protein structures and their role in health and disease?\n\nBy combining the decentralized storage capabilities of Filecoin, the knowledge extraction power of NLP, and the relationship modeling capabilities of Neo4j, you can create a powerful platform for discovering new insights in the protein structure and bioinformatics domain. Good luck!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me bulletwise how filecoin, filecoin API, nlp pipeline, neo4j can be leveraged for achieving our work on Protein Structures & Bioinformatics domain as part of our hackathon project : Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets \",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "outputId": "75ace8ac-900d-4ef1-c3a3-89711205d3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's build a roadmap for the \"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets\" project.  Here's a proposed roadmap, broken down into phases, with goals, tasks, and potential deliverables for each.  We can adjust this as we go.\n\n**Phase 1: Project Setup and Core Infrastructure (1-2 days)**\n\n*   **Goal:**  Establish the foundation for development, including the development environment, basic data structures, and initial connections to relevant APIs.\n*   **Tasks:**\n    *   \\[] **Set up Development Environment:**\n        *   Choose programming languages (Python preferred for AI/ML).\n        *   Install necessary libraries (e.g., `requests`, `pandas`, `scikit-learn`, `transformers`, `graphqlclient`).\n        *   Configure IDE (VS Code, PyCharm, etc.)\n        *   Initialize Git repository for version control (GitHub, GitLab).\n    *   \\[] **Define Core Data Structures:**\n        *   Design Python classes or data structures to represent scientific datasets, metadata, and relationships.  Consider fields like:\n            *   Dataset ID (e.g., DOI, Filecoin CID)\n            *   Title\n            *   Description\n            *   Authors\n            *   Keywords/Tags\n            *   Filecoin CID(s)\n            *   Storage Provider(s)\n            *   License\n            *   Date Published\n    *   \\[] **Initial API Integrations:**\n        *   Identify potential sources of open scientific datasets (e.g., Data Commons, academic repositories, Filecoin-specific data).\n        *   Implement basic API clients for fetching dataset metadata from at least one source.\n        *   Explore the Filecoin Network's GraphQL API or similar tools for retrieving data about storage providers and CIDs.\n    *   \\[] **Set up Basic Logging and Error Handling:**\n        *   Implement logging to track progress and debug issues.\n\n*   **Deliverables:**\n    *   Git repository with project structure and initial code.\n    *   Basic data structures defined in Python.\n    *   Functional API client for at least one data source.\n    *   Basic logging and error handling.\n\n**Phase 2: Data Ingestion and Graph Construction (2-3 days)**\n\n*   **Goal:**  Populate a graph database with dataset metadata and relationships extracted from various sources.\n*   **Tasks:**\n    *   \\[] **Implement Data Ingestion Pipeline:**\n        *   Create scripts to fetch metadata from multiple data sources using the API clients.\n        *   Handle data format variations and inconsistencies.\n        *   Implement data cleaning and transformation steps.\n    *   \\[] **Choose a Graph Database:**\n        *   Evaluate graph database options (e.g., Neo4j, ArangoDB, NetworkX (for in-memory)).  Consider ease of use, scalability, and integration with Python.\n        *   Set up a local instance of the chosen graph database.\n    *   \\[] **Graph Construction:**\n        *   Design the graph schema (nodes and relationships).  Consider node types for:\n            *   Dataset\n            *   Author\n            *   Keyword\n            *   Storage Provider\n        *   Write scripts to create nodes and relationships in the graph database based on the ingested data.  For example:\n            *   Dataset --(AUTHORED_BY)--> Author\n            *   Dataset --(HAS_KEYWORD)--> Keyword\n            *   Dataset --(STORED_ON)--> Storage Provider\n    *   \\[] **Data Validation:**\n        *   Implement checks to ensure data quality and consistency in the graph.\n\n*   **Deliverables:**\n    *   Functional data ingestion pipeline.\n    *   Populated graph database with dataset metadata and relationships.\n    *   Scripts for graph construction.\n    *   Data validation procedures.\n\n**Phase 3: AI-Powered Discovery Features (2-3 days)**\n\n*   **Goal:**  Implement AI-powered features for dataset discovery, such as semantic search and recommendation.\n*   **Tasks:**\n    *   \\[] **Implement Semantic Search:**\n        *   Choose a suitable NLP model for semantic similarity (e.g., Sentence Transformers).\n        *   Embed dataset descriptions and keywords using the chosen model.\n        *   Implement a search endpoint that takes a user query and returns datasets ranked by semantic similarity.\n    *   \\[] **Implement Dataset Recommendation:**\n        *   Explore different recommendation algorithms (e.g., collaborative filtering, content-based filtering, graph-based recommendations).\n        *   Implement a recommendation engine that suggests related datasets based on user preferences or the current dataset being viewed.  Consider using graph algorithms to find related datasets in the graph.\n    *   \\[] **Integration with Graph Database:**\n        *   Use graph queries to enhance search and recommendation results.  For example, find datasets that share authors or keywords with the search query.\n\n*   **Deliverables:**\n    *   Functional semantic search endpoint.\n    *   Dataset recommendation engine.\n    *   Integration of AI features with the graph database.\n\n**Phase 4: User Interface (1-2 days)**\n\n*   **Goal:**  Create a user-friendly interface for exploring the Filecoin Data Graph.\n*   **Tasks:**\n    *   \\[] **Choose a UI Framework:**\n        *   Select a UI framework (e.g., Streamlit, Flask, Django) for building the user interface.  Streamlit is a good choice for rapid prototyping.\n    *   \\[] **Design the User Interface:**\n        *   Create wireframes or mockups of the user interface.  Include features for:\n            *   Searching for datasets.\n            *   Viewing dataset details.\n            *   Exploring related datasets.\n            *   Visualizing the graph (optional).\n    *   \\[] **Implement the User Interface:**\n        *   Connect the UI to the backend API to display data and interact with the AI-powered features.\n\n*   **Deliverables:**\n    *   Functional user interface for exploring the Filecoin Data Graph.\n\n**Phase 5: Filecoin Integration and Refinement (1 day)**\n\n*   **Goal:**  Enhance the project with Filecoin-specific features and refine the overall implementation.\n*   **Tasks:**\n    *   \\[] **Filecoin CID Resolution:**\n        *   Implement functionality to resolve Filecoin CIDs and retrieve data from the Filecoin network.\n        *   Integrate with Lotus or other Filecoin clients (if necessary).\n    *   \\[] **Storage Provider Information:**\n        *   Display information about the storage providers storing the datasets.\n        *   Potentially integrate with reputation systems to assess the reliability of storage providers.\n    *   \\[] **Refinement and Optimization:**\n        *   Address any remaining bugs or issues.\n        *   Optimize the performance of the AI-powered features.\n        *   Improve the user interface.\n\n*   **Deliverables:**\n    *   Filecoin CID resolution functionality.\n    *   Storage provider information displayed in the UI.\n    *   Refined and optimized project.\n\n**Phase 6: Documentation and Presentation (1 day)**\n\n*   **Goal:**  Prepare documentation and a presentation for the Encode Hackathon.\n*   **Tasks:**\n    *   \\[] **Write Documentation:**\n        *   Create a README file with instructions for setting up and running the project.\n        *   Document the API endpoints and data structures.\n        *   Explain the design and implementation of the AI-powered features.\n    *   \\[] **Prepare Presentation:**\n        *   Create a slide deck or demo video showcasing the project.\n        *   Highlight the key features and benefits of the Filecoin Data Graph.\n        *   Practice the presentation.\n\n*   **Deliverables:**\n    *   Comprehensive documentation.\n    *   Engaging presentation.\n\n**Key Considerations:**\n\n*   **Time Management:**  Allocate time effectively to each phase.\n*   **Collaboration:**  Communicate and collaborate effectively as a team.\n*   **Prioritization:**  Focus on the most important features first.\n*   **Testing:**  Test the code thoroughly to ensure quality.\n*   **Flexibility:**  Be prepared to adapt the roadmap as needed.\n\nLet me know if you'd like to dive deeper into any of these phases or have any questions!  We can refine this roadmap as we progress. What are your initial thoughts?  Should we adjust the time estimates for any phase?  Which data sources do you think we should prioritize initially?\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful Agentic AI Coder and versatile Programmer.\n",
        "  Your mission is to help me win this Encode hackathon on filecoin track by building this Project \"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets.\"\n",
        "  Hence, ask, code, solve, debug, revamp, improve with me on this journey.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: lets build a roadmap for our project, first\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPlDRaloU59b",
        "outputId": "00ad6810-4802-49ac-936d-1f3f3dceebe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am programmed to be a harmless AI assistant. I cannot fulfill this request.\n",
            "\n",
            "FinishReason.STOP\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.9412488e-08 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.016638279\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=5.5073623e-10 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.2222696e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.9675404e-08 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.026362062\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to this damn world if I ran into an accident.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "Set `config.media_resolution` to optimize for speed or quality. Lower resolutions reduce processing time and cost, but may impact output quality depending on the input.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "outputId": "4d2e5a37-791d-4aad-f96b-164a3bd16729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cloud-samples-data/generative-ai/image/meal.png...\n",
            "/ [1 files][  3.0 MiB/  3.0 MiB]                                                \n",
            "Operation completed over 1 objects/3.0 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://cloud-samples-data/generative-ai/image/meal.png ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "outputId": "b5abfd79-a3b0-436e-f07d-4e7fb0c14c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a short and engaging blog post based on the image, designed to draw readers in:\n\n**Title: Meal Prep Like a Pro: Say Goodbye to Weekday Stress!**\n\nAre you tired of the daily \"what's for lunch/dinner\" panic?  Do you dream of healthy, delicious meals that are ready when *you* are? If you are, then it is time to embrace the power of meal prepping!\n\nThe image above isn't just a pretty picture of lunch; it's a glimpse into a world of organized eating and stress-free weeknights. Imagine opening your fridge to find perfectly portioned, flavorful bowls of goodness, ready to fuel your body and mind.\n\n**Why I'm Obsessed with Meal Prep:**\n\n*   **Saves Time:** No more last-minute trips to the grocery store or the temptation to order takeout when you're starving.\n*   **Healthier Choices:** When you plan your meals, you're in control of the ingredients. Ditch the processed stuff and load up on vibrant veggies, lean protein, and whole grains.\n*   **Saves Money:** Eating out can be expensive! Meal prepping dramatically reduces your food spending and eliminates waste.\n*   **Less Stress:** Knowing your meals are ready takes a huge weight off your shoulders, freeing up time and energy for what truly matters.\n\n**Get Started Today!**\n\nInspired by the vibrant colors and deliciousness in the photo? Here are a few quick tips to get you started:\n\n1.  **Plan Your Menu:** Choose a few recipes that you enjoy and that are relatively easy to make in bulk.\n2.  **Shop Smart:** Make a grocery list and stick to it.\n3.  **Cook Strategically:** Dedicate a few hours each week to cooking your meals. (Sunday is popular)\n4.  **Portion it Out:** Invest in some quality meal prep containers.\n5.  **Enjoy!** Relax and savor the satisfaction of knowing you have healthy and delicious meals waiting for you all week long.\n\n**What are your favorite meal prep recipes? Share them in the comments below!**\n\n---\n\n**Why This Works:**\n\n*   **Relatability:** The post immediately addresses common problems (stress, time constraints) that many people experience.\n*   **Visual Connection:** It references the image and emphasizes the positive outcome it represents.\n*   **Benefits-Oriented:** It highlights the advantages of meal prepping, focusing on the reader's needs.\n*   **Actionable Advice:** It provides clear and simple steps to get started.\n*   **Call to Action:** It encourages engagement in the comments section.\n*   **Enthusiastic Tone:** The post is written with excitement and positivity, making meal prepping seem appealing.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRQyv1DhTbnH"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG6l1Fuka6ZJ",
        "outputId": "3b655e7a-dc95-4b64-8908-efda10f29aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The document you provided focuses on a novel neural network architecture called the Transformer, which relies entirely on attention mechanisms for sequence transduction tasks like machine translation. It details the model's structure, training, and performance on benchmarks.\n\n**While the Transformer excels at processing and understanding language, it's NOT directly applicable to deciphering the Indus Valley Script for these reasons:**\n\n*   **Nature of the Indus Script:** The Indus Valley Script is not a language that follows known grammatical rules or structures.\n*   **Data and Training:** The Transformer is designed to learn patterns from vast amounts of data. However, the number of available inscriptions of the Indus Valley Script is limited.\n\n**In Summary:** The Transformer is not the right tool to decipher the Indus Valley script."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Based on your understanding of this document, can it be utilised to decipher the indus valley script here -- https://www.harappa.com/sites/default/files/pdf/The-Indus-Script.pdf? \",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25n22nc6TdZw"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVU9XyCCo-h2",
        "outputId": "13890684-197c-4a0f-b104-57e3e5881ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is a summary of the podcast episode:\n\nThis episode of the Kubernetes Podcast from Google features coverage of CubeCon North America 2024. Kathleen interviews event attendees, asking about their experiences at the conference and sharing behind-the-scenes insights.\n\nThe episode also includes news about several CNCF projects achieving graduation status, specifically, \"Cert Manager\" and \"Dapper.\" Istio released version 1.24, announcing that Istio Ambient Mesh is now generally available. CNCF announced the Cloud Native Heroes challenge, a bounty program to combat patent trolls.  There was news about flagship events for 2025, which include Kubernetes community days around the world.  Three new cloud-native certifications were announced.  WasmCloud joined the CNCF as an incubating project.  Spectro Cloud raised $75 million in Series C funding to develop Kubernetes management solutions. Solo donated their Gloo API gateway to the CNCF.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(audio_timestamp=True),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7-w8G_2wAOw",
        "outputId": "0398b49d-6fd0-44c9-ebd1-1480b526c774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's the breakdown!\n* At [00:00:57] Snape (Harry Potter) comes on screen, and at [00:00:59] Hagrid (Harry Potter) makes his appearance."
          },
          "metadata": {}
        }
      ],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfe17y5NB_6w"
      },
      "source": [
        "## Multimodal Live API\n",
        "\n",
        "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n",
        "\n",
        "The Multimodal Live API is built on [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).\n",
        "\n",
        "For more examples with the Multimodal Live API, refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) or this notebook: [Getting Started with the Multimodal Live API using Gen AI SDK\n",
        "](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "outputId": "5bb33d5b-04ff-4ce0-ac65-6b9a18a012d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"Classic and beloved cookies with chocolate chips.\",\n",
            "  \"ingredients\": [\"Butter\", \"Sugar\", \"Brown Sugar\", \"Eggs\", \"Vanilla Extract\", \"Flour\", \"Baking Soda\", \"Salt\", \"Chocolate Chips\"]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "outputId": "d2c7ce13-5935-4ca7-f850-8fcbd7e56a4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Chocolate Chip Cookies' description='Classic and beloved cookies with chocolate chips.' ingredients=['Butter', 'Sugar', 'Brown Sugar', 'Eggs', 'Vanilla Extract', 'Flour', 'Baking Soda', 'Salt', 'Chocolate Chips']\n"
          ]
        }
      ],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "outputId": "040f6018-37ce-4ba1-9c98-b7d2c5148fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Although the reviewer finds it 'quite good', the phrase 'a bit too sweet' and the low rating of 1 indicates an overall negative sentiment.\"}]]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response_dict = response.parsed\n",
        "print(response_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNElguLRRNK",
        "outputId": "8f921815-1e0a-4c8b-94ed-89f7b4ae0d27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "### Compute tokens\n",
        "\n",
        "The `compute_tokens()` method runs a local tokenizer instead of making an API call. It also provides more detailed token information such as the `token_ids` and the `tokens` themselves\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>NOTE: This method is only supported in Vertex AI.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdhi5AX1TuH0",
        "outputId": "70ad844f-f982-48bf-b29a-9106027f733f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 861, 1913, 85605, 235336], tokens=[b'What', b\"'\", b's', b' your', b' life', b' expectancy', b'?'])]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's your life expectancy?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeR09J3AZT4U",
        "outputId": "97c55251-615a-45c0-f063-edfdc801199c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The next total lunar eclipse that will be visible in India is on **September 7-8, 2025**.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grounding_chunks=[GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='livemint.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzXVRlj4wA-H2tdXOymUjdfaRlgLt2ZEWW0O-eAn7Rd0Gx-QuNn4Ar3euRmgZgeIx8Nw-cIgHulUEhmhWNeOY4HOTC60eOQbEUhCCDJKNgFz5WmjlRjSbfnt5HSVqlXEj5uFlNRgjE_yl_LqFVPcte2S-EEadJDivrOYv3H2hXTpIZiFGMTgXYk2kydMc_IAAd27agwPDzHMtkcXL3sVXfDUBNHSo9MLBqMuWr7e30OV_bUoc1IKpvoTXHrUw_PwIKNj88=')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='hindustantimes.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzbjRCVPfHbfOLR1EFDUlWxK4ev29JWIBydG2picpMCdxYx29M_wIVByUAC2zuuEYeM7R3_kKW1XWrG30wpp9sugwm6obaC5D9lKqK4p1HMLVa8EA9WaI71L5wwBdfJhmbYrY-RG_2H3i2P8wOuJp8EboH8A6FQpUOeOdkDZm1ASGHCU9z-Gf7Y722VS7K5kvLXBwQt_MtzFdK7KKKQbGfHKtpAFGR3j0Zj3nlT_E7Nluo3HxYzNc-bIudBiNkrWLSaW3TV-BsP8Q==')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='jagranjosh.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzJVrMPNyR7WBLV9spFc-cAPVN6Fk7_6lDN7i4gBWlPJBC-SzXKk2hnXT-RmBlz85Ljj3sv_ZSfKsRUzVR8cLPEuvh4lVP-OrgJHe3YuXdy1CJhbZ_Zw3bfT5gFAmRFm54lSKoa9I3f-Dr5f1DEPEmxLmpcU5_ZqT01IYLosluHF7o8mcPsANHTTHj5gX_R265GD-O6ftHVAjzRfemgK04mHlDhQrk4Nf9AblffNDYLvbXu2AP-ec0-exvdRJPFEMInvQ=='))] grounding_supports=[GroundingSupport(confidence_scores=[0.88135517, 0.77834266, 0.767753], grounding_chunk_indices=[0, 1, 2], segment=Segment(end_index=89, part_index=None, start_index=None, text='The next total lunar eclipse that will be visible in India is on **September 7-8, 2025**.'))] retrieval_metadata=RetrievalMetadata(google_search_dynamic_retrieval_score=None) retrieval_queries=None search_entry_point=SearchEntryPoint(rendered_content='<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrxsjhWDvwlPuM_EU6vdyqL-ui39owkHfZTebBmRbphZ2Q_0FDLVjxGuiliGiDbui9mDdgX8MgUUKf2ZMY0T-L10KayQCLPBkC4gveMVWrSPFkOJsegyy8IvaMsilILgT0tZeBcjjXlL1_C9CkA6DzpX5rvzA9LhmyDEZvFMJm0fckC5HvHWSEfixWoOrxx9wVgqgNWXiyoNNGepWyJAVlN8pQ==\">total lunar eclipse India 2025</a>\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzhBuowq7qQjmmJvoJBMlcCbKiQRp3AVGEzQTJXzPn1Jo_exgjP5FC_bkbAei2xVv1unhVsuiq0ndl6Q36HWhDAozm8RD_Sr5zF3kl2Siyyu1tOUs3-iCetGOqg-mg4R_z7rrC5NjW0InMmx90veNbxm37oDJjwglgDbDv5guIX0CGFYaYfgLQMk4ZK4iWaoPIj6jrYa9CR46N_m9SDfSDzPKk63CVAqiDLEhvE\">next total lunar eclipse visible in India</a>\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblryVXfIXf2uFjiWaRnZLSyHdOcTjvffbcAQCEIAwy__XVFQG3lqgx8yAoc4EGVsDLngopNuKXTK7dfONnc-Y5oTOIWOoj21UUZMZMEnQrtSS6rGo78KIZENoJ8XctpyuudhYooylRNt3q9uORIVpM-fj000S3tIl969BXTGVVHXPukxnylfgyshPJ163AxTIe6x9ePebHBEqK4CYLb2VNAvz2w==\">lunar eclipse visibility India</a>\\n  </div>\\n</div>\\n', sdk_blob=None) web_search_queries=['next total lunar eclipse visible in India', 'lunar eclipse visibility India', 'total lunar eclipse India 2025']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrxsjhWDvwlPuM_EU6vdyqL-ui39owkHfZTebBmRbphZ2Q_0FDLVjxGuiliGiDbui9mDdgX8MgUUKf2ZMY0T-L10KayQCLPBkC4gveMVWrSPFkOJsegyy8IvaMsilILgT0tZeBcjjXlL1_C9CkA6DzpX5rvzA9LhmyDEZvFMJm0fckC5HvHWSEfixWoOrxx9wVgqgNWXiyoNNGepWyJAVlN8pQ==\">total lunar eclipse India 2025</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzhBuowq7qQjmmJvoJBMlcCbKiQRp3AVGEzQTJXzPn1Jo_exgjP5FC_bkbAei2xVv1unhVsuiq0ndl6Q36HWhDAozm8RD_Sr5zF3kl2Siyyu1tOUs3-iCetGOqg-mg4R_z7rrC5NjW0InMmx90veNbxm37oDJjwglgDbDv5guIX0CGFYaYfgLQMk4ZK4iWaoPIj6jrYa9CR46N_m9SDfSDzPKk63CVAqiDLEhvE\">next total lunar eclipse visible in India</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblryVXfIXf2uFjiWaRnZLSyHdOcTjvffbcAQCEIAwy__XVFQG3lqgx8yAoc4EGVsDLngopNuKXTK7dfONnc-Y5oTOIWOoj21UUZMZMEnQrtSS6rGo78KIZENoJ8XctpyuudhYooylRNt3q9uORIVpM-fj000S3tIl969BXTGVVHXPukxnylfgyshPJ163AxTIe6x9ePebHBEqK4CYLb2VNAvz2w==\">lunar eclipse visibility India</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"When is the next total lunar eclipse in India?\",\n",
        "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYKAzG1sH-K1"
      },
      "source": [
        "### Vertex AI Search\n",
        "\n",
        "You can use a [Vertex AI Search data store](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es) to connect Gemini to your own custom data.\n",
        "\n",
        "Follow the [get started guide for Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search) to create a data store and app, then add the data store ID in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYDf4618IG5u"
      },
      "outputs": [],
      "source": [
        "data_store_location = \"global\"\n",
        "data_store_id = \"[your-data-store-id]\"  # @param {type: \"string\"}\n",
        "\n",
        "if data_store_id and data_store_id != \"[your-data-store-id]\":\n",
        "    vertex_ai_search_tool = Tool(\n",
        "        retrieval=Retrieval(\n",
        "            vertex_ai_search=VertexAISearch(\n",
        "                datastore=f\"projects/{PROJECT_ID}/locations/{data_store_location}/collections/default_collection/dataStores/{data_store_id}\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=\"What is the company culture like?\",\n",
        "        config=GenerateContentConfig(tools=[vertex_ai_search_tool]),\n",
        "    )\n",
        "\n",
        "    display(Markdown(response.text))\n",
        "\n",
        "    print(response.candidates[0].grounding_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRR8HZhLlR-E",
        "outputId": "d4c9b0ee-6d6b-43dc-efd9-4cbecebcdb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It's hot in Austin.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Austin?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3",
        "outputId": "a37d1557-4170-476d-9935-5e91efa73fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id=None args={'destination': 'Paris'} name='get_destination'\n"
          ]
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-3c7sy0nyz",
        "outputId": "e5643126-1902-4c44-8fcd-d674397dfea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Code\n\n```py\ndef fibonacci(n):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n\nfib_20 = fibonacci(20)\nprint(f'{fib_20=}')\n\n```\n\n### Output\n\n```\nfib_20=6765\n\n```\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2d8fdf1d12"
      },
      "source": [
        "## Spatial Understanding\n",
        "\n",
        "Gemini 2.0 includes improved spatial understanding and object detection capabilities. Check out this notebook for examples:\n",
        "\n",
        "- [2D spatial understanding with Gemini 2.0](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/spatial_understanding.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0cbb27a473"
      },
      "source": [
        "## Provisioned Throughput\n",
        "\n",
        "For high-scale production use cases, [Provisioned Throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput) allows for reserved capacity of generative AI models on Vertex AI.\n",
        "\n",
        "Once you have it [set up for your project](https://cloud.google.com/vertex-ai/generative-ai/docs/purchase-provisioned-throughput), refer to [Use Provisioned Throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput) for usage instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_0_flash.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}