{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nkovaturient/Google-Doodle-Frontend/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.0 Flash\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://goo.gle/40JXy6g\">\n",
        "      <img width=\"32px\" src=\"https://cdn.qwiklabs.com/assets/gcp_cloud-e3a77215f0b8bfa9b3f611c0d2208c7e8708ed31.svg\" alt=\"Google Cloud logo\"><br> Open in Cloud Skills Boost\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) |  [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is available through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
        "\n",
        "- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
        "- Speed and performance: Gemini 2.0 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.\n",
        "- Quality: The model maintains quality comparable to larger models like Gemini 1.5 Pro and GPT-4o.\n",
        "- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
        "- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.\n",
        "- To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API in Vertex AI and the Google Gen AI SDK for Python with the Gemini 2.0 Flash model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text from text prompts\n",
        "  - Generate streaming text\n",
        "  - Start multi-turn chats\n",
        "  - Use asynchronous methods\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgernCnR_xlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "c02ed1d1-8d11-4666-ca29-47380bbdb293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m143.4/144.7 kB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services.\n",
        "\n",
        "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymmEN6GSTn-"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    *   Run the cell below to set your project ID.\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    *   Run the cell further below to use your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1933326c939"
      },
      "source": [
        "#### Option 1. Use a Google Cloud Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UCgUOv4nSWhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"hack2skill-project\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aa38ee3158"
      },
      "source": [
        "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
        "\n",
        "Uncomment the following block to use Express Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpIPG_YhSjaw"
      },
      "outputs": [],
      "source": [
        "# API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
        "\n",
        "# if not API_KEY or API_KEY == \"[your-api-key]\":\n",
        "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
        "\n",
        "# client = genai.Client(vertexai=True, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36ce4ac022"
      },
      "source": [
        "Verify which mode you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8338643f335f",
        "outputId": "67427c15-67c5-451e-d649-a8084b2c8393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vertex AI with project: hack2skill-project in location: us-central1\n"
          ]
        }
      ],
      "source": [
        "if not client.vertexai:\n",
        "    print(f\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.0 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.0 Flash model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "7c97254d-7c81-482b-db18-4a718842e897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break down how to win the Encode Club AI Blueprints hackathon, focusing on Filecoin and Randamu projects, while tackling complexities with simple and innovative solutions.\n\n**Overall Strategy: Simplicity, Innovation, and Impact**\n\nThe key to winning this hackathon isn't just building *something* that works; it's building *something useful, easily explainable, technically sound, and innovative* that tackles a real problem.\n\nHere's a comprehensive roadmap with architectural workflow diagrams.  I'll address both Filecoin and Randamu separately, but keep in mind that the best projects might cleverly combine aspects of both or use one to enhance the other.\n\n**I. Project Roadmap (General)**\n\n1.  **Understand the Problem Space:**\n    *   **Deep Dive:**  Go beyond the hackathon description. Research Filecoin, Randamu, AI, and related challenges. Look for real-world use cases where these technologies can make a difference.  Think about data storage, AI model training, verifiable randomness, data marketplaces, and decentralized AI.\n    *   **Identify a Pain Point:** Find a specific, well-defined problem that can be realistically addressed within the hackathon timeframe. Avoid scope creep! A focused, well-executed solution is better than a broad, buggy one.\n\n2.  **Brainstorm Ideas (Innovation):**\n    *   **Think Outside the Box:** Don't just replicate existing solutions.  Consider novel combinations of technologies. Look for areas where AI can automate or optimize existing processes related to Filecoin or Randamu.\n    *   **Focus on Usability:** How easy is your solution to use? Can a non-technical person understand the value proposition?  Think about user interface (UI) and user experience (UX).\n\n3.  **Choose the Best Idea:**\n    *   **Feasibility:** Can you build it in the time allotted?\n    *   **Impact:** Does it solve a real problem?  How much value does it create?\n    *   **Innovation:** Is it unique and creative?\n    *   **Technical Fit:** Does it leverage the strengths of Filecoin and/or Randamu?\n    *   **Presentation:** Can you clearly explain the problem, your solution, and its benefits in a compelling way?\n\n4.  **Design and Architecture (Simplicity):**\n    *   **Keep it Simple:**  Start with a minimal viable product (MVP).  Focus on core functionality.  Avoid over-engineering.\n    *   **Modular Design:** Break your project into smaller, manageable components.  This makes development, testing, and debugging easier.\n    *   **Clear Documentation:**  Document your architecture, code, and usage.  This is crucial for judges to understand your project.\n\n5.  **Development and Testing:**\n    *   **Version Control (Git):** Use Git for version control.  Commit frequently and use meaningful commit messages.\n    *   **Testing:**  Write unit tests and integration tests to ensure your code works correctly.\n    *   **Debugging:**  Learn how to use debugging tools to quickly identify and fix issues.\n\n6.  **Presentation and Demo:**\n    *   **Clear and Concise:**  Explain your project in a way that anyone can understand.\n    *   **Compelling Story:**  Tell a story about the problem you're solving and how your solution addresses it.\n    *   **Live Demo:**  Show your project in action.  Prepare a well-rehearsed demo that highlights the key features.\n    *   **Visual Aids:**  Use diagrams, screenshots, and videos to enhance your presentation.\n    *   **Q&A:**  Be prepared to answer questions from the judges.\n\n**II. Filecoin Project Ideas and Architecture**\n\n**Idea 1: AI-Powered Filecoin Storage Optimization**\n\n*   **Problem:**  Users often store data on Filecoin without knowing the optimal parameters for storage (e.g., deal duration, replication factor, retrieval pricing).  This can lead to inefficiencies and higher costs.\n\n*   **Solution:** An AI model that analyzes user data and recommends optimal storage configurations based on factors like data size, data type, access frequency, and budget.\n\n*   **Innovation:**  Automates the process of finding the best storage deals, reducing costs and improving storage efficiency.\n\n*   **Filecoin Technologies Used:** Filecoin Storage Market, Filecoin Retrieval Market, Filecoin API.\n\n*   **AI Technologies Used:** Machine Learning (e.g., Regression, Classification), Data Analysis, Recommendation Engines.\n\n*   **Architecture Diagram:**\n\n```mermaid\ngraph LR\n    A[User Data] --> B(AI Model: Storage Optimizer);\n    B --> C{Decision: Optimal Storage Configuration};\n    C --> D[Filecoin API: Storage Market Interaction];\n    D --> E[Filecoin Network: Storage Providers];\n    E --> F[Data Stored on Filecoin];\n    F --> G[User Data Retrieval];\n    G --> H[Filecoin Retrieval Market];\n    H --> A;\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n*   **Workflow:**\n    1.  User provides data characteristics (e.g., file size, type, access frequency, budget).\n    2.  AI model analyzes the data and historical Filecoin market data.\n    3.  AI model recommends optimal storage configurations (deal duration, replication factor, price).\n    4.  The system automatically interacts with the Filecoin Storage Market to create a storage deal with the recommended parameters.\n    5.  Data is stored on Filecoin.\n    6. When the User wants to retrieve the data it goes from Filecoin retrieval market\n\n**Idea 2:  AI-Driven Content Addressable Data Curation and Discovery**\n\n*   **Problem:**  Filecoin uses content addressing, but it can be difficult to find specific data if you don't know the CID (Content Identifier).  Metadata is essential for discovery.\n\n*   **Solution:** An AI-powered system that automatically extracts metadata from files stored on Filecoin and creates a searchable index.  This could use natural language processing (NLP) and computer vision to understand the content of the files.\n\n*   **Innovation:**  Makes data stored on Filecoin more discoverable and accessible.\n\n*   **Filecoin Technologies Used:**  Filecoin Content Addressing (CID), IPFS (InterPlanetary File System), Libp2p.\n\n*   **AI Technologies Used:**  Natural Language Processing (NLP), Computer Vision, Machine Learning (for classification and clustering), Metadata Extraction.\n\n*   **Architecture Diagram:**\n\n```mermaid\ngraph LR\n    A[File Stored on Filecoin] --> B{IPFS/Filecoin Network};\n    B --> C[Metadata Extraction (AI Model)];\n    C --> D[Metadata Index (Searchable Database)];\n    E[User Query] --> F[Search Engine];\n    F --> D;\n    D --> G[Results (File CIDs)];\n    G --> B;\n    B --> H[Return File to User];\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style H fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n*   **Workflow:**\n    1.  A file is stored on Filecoin, accessible via IPFS.\n    2.  The AI-powered system retrieves the file from IPFS.\n    3.  AI models extract metadata from the file (e.g., title, author, keywords, image descriptions).\n    4.  The metadata is stored in a searchable index.\n    5.  Users can search the index using keywords or natural language queries.\n    6.  The search engine returns the CIDs of relevant files.\n    7.  Users can then retrieve the files from Filecoin/IPFS using the CIDs.\n\n**III. Randamu Project Ideas and Architecture**\n\n**Idea 1:  Verifiable Randomness for Decentralized AI Model Selection**\n\n*   **Problem:**  In decentralized AI, you might have multiple models trained by different parties.  You need a fair and verifiable way to choose which model to use for a particular task.\n\n*   **Solution:** Use Randamu to provide verifiable randomness for selecting an AI model from a pool of available models.\n\n*   **Innovation:**  Ensures fairness and transparency in decentralized AI model selection.\n\n*   **Randamu Technologies Used:**  Randamu API, Randamu Verifiable Random Function (VRF).\n\n*   **AI Technologies Used:**  Decentralized AI, Model Evaluation Metrics.\n\n*   **Architecture Diagram:**\n\n```mermaid\ngraph LR\n    A[List of AI Models] --> B(Randamu API: Request Randomness);\n    B --> C[Randamu Network];\n    C --> D{Verifiable Random Value};\n    D --> E[Model Selection Algorithm];\n    E --> F[Selected AI Model];\n    F --> G[AI Inference];\n    G --> H[Result];\n    style H fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n*   **Workflow:**\n    1.  A list of available AI models is compiled.\n    2.  The system requests a random value from the Randamu API.\n    3.  Randamu generates a verifiable random value.\n    4.  A model selection algorithm uses the random value to choose a model from the list.  The algorithm could be as simple as selecting the model at the index corresponding to the random value modulo the number of models.\n    5.  The selected AI model is used for inference.\n    6.  The results of the AI inference are returned. The verifiability of Randamu ensures no bias in model selection.\n\n**Idea 2:  Randamu-Powered Secure and Fair Federated Learning**\n\n*   **Problem:** Federated learning involves training AI models on decentralized data sources.  However, data privacy and security are major concerns.  There's also a risk that malicious participants could manipulate the training process.\n\n*   **Solution:** Use Randamu to randomly select participants in each round of federated learning and to add noise to the model updates, enhancing privacy and preventing manipulation.\n\n*   **Innovation:**  Improves the security and fairness of federated learning.\n\n*   **Randamu Technologies Used:**  Randamu API, Randamu VRF.\n\n*   **AI Technologies Used:**  Federated Learning, Differential Privacy.\n\n*   **Architecture Diagram:**\n\n```mermaid\ngraph LR\n    A[List of Participants] --> B(Randamu API: Random Participant Selection);\n    B --> C[Randamu Network];\n    C --> D{Randomly Selected Participants};\n    D --> E[Local Model Training];\n    E --> F[Model Updates];\n    F --> G(Add Noise (Differential Privacy));\n    G --> H[Aggregate Model Updates];\n    H --> I[Global Model Update];\n    I --> A;\n    style I fill:#f9f,stroke:#333,stroke-width:2px\n```\n\n*   **Workflow:**\n    1.  A list of participants is compiled.\n    2.  Randamu is used to randomly select a subset of participants for each round of federated learning.\n    3.  The selected participants train the model locally on their data.\n    4.  Participants submit model updates.\n    5.  Noise is added to the model updates using differential privacy techniques. The randomness from Randamu can also be used as part of the noise generation process.\n    6.  The model updates are aggregated to create a global model update.\n    7.  The global model is updated and the process repeats.  The verifiable randomness ensures fair and secure participant selection.\n\n**IV. Winning Tips**\n\n*   **Prioritize a Working Demo:**  Judges want to see a functional demo, even if it's basic.\n*   **Focus on Impact:**  Clearly articulate the problem you're solving and the value of your solution.\n*   **Highlight Innovation:**  Explain what makes your project unique and creative.\n*   **Prepare a Great Presentation:**  Practice your presentation and make sure it's clear, concise, and engaging.\n*   **Teamwork:** If you're working in a team, make sure everyone knows their role and responsibilities.\n*   **Use the Filecoin and Randamu APIs Effectively:** Show that you understand how to use these technologies.\n*   **Document Everything:**  Provide clear documentation of your code, architecture, and usage.\n*   **Ask for Help:** Don't be afraid to ask for help from the mentors and community.\n*   **Have Fun!**  The most important thing is to learn and have fun.\n\n**V. Tools and Technologies**\n\n*   **Filecoin:** Filecoin API, Lotus (Filecoin implementation), IPFS.\n*   **Randamu:** Randamu API.\n*   **AI/ML:** Python (TensorFlow, PyTorch, Scikit-learn), Jupyter Notebooks, Cloud ML platforms (Google Cloud AI Platform, AWS SageMaker).\n*   **Web Development:** JavaScript (React, Vue.js, Angular), HTML, CSS.\n*   **Smart Contracts:** Solidity (if you need to interact with the Filecoin blockchain).\n*   **Development Environment:**  VS Code, Git, Docker.\n\n**VI. How to Keep It Simple**\n\n*   **Start Small:** Focus on building a minimal viable product (MVP) with core functionality.\n*   **Use Existing Libraries and Frameworks:** Don't reinvent the wheel.  Leverage existing tools and libraries to speed up development.\n*   **Modular Design:** Break your project into smaller, independent modules.\n*   **Clear Code:** Write clean, well-documented code that is easy to understand.\n*   **Focus on One Problem:**  Don't try to solve too many problems at once.\n*   **Iterate:** Start with a basic solution and then iteratively improve it.\n\n**VII. Ethical Considerations**\n\n*   **Data Privacy:**  Be mindful of data privacy and security.  Use techniques like differential privacy to protect user data.\n*   **Bias:** Be aware of potential biases in your AI models and take steps to mitigate them.\n*   **Transparency:**  Make your AI models and algorithms as transparent as possible.\n\nBy following this roadmap and focusing on simplicity, innovation, and impact, you'll be well-positioned to win the Encode Club AI Blueprints hackathon with a Filecoin or Randamu project. Good luck!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"how to win this hackathon under filecoin and randamu project given in this link- https://www.encode.club/ai-blueprints solving the complexities and challenges in a simple, innovative way.?ensure u provide me with a comprehensive, clear roadmap and architectural workflow diagrammatic representation for each project\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "e212b048-558c-43c0-c6b3-624326d5897d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ", let'"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "s break down the potential of your idea for the Encode Hackathon, and how"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " to strategically position it to maximize your chances of success.\n\n**Overall Assessment:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Good, but needs refinement to be \"mindblowing\" and a strategic hackathon win.**\n\nHere's a breakdown of the pros, cons, and how to level"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " it up:\n\n**Pros:**\n\n*   **Relevant Problem:** You've identified a genuine pain point in the Filecoin ecosystem: discoverability.  File"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "coin's strength (content addressing) can also be a weakness without good metadata.  Hackathon judges will appreciate you targeting a real-world challenge.\n*   **Combines Hot Technologies:** AI and decentralized storage are both buzzwords that"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " attract attention. Marrying them is generally a good strategy.\n*   **Clear Technical Implementation:** You've outlined the key technologies involved, demonstrating you've thought about the practical aspects.\n*   **Filecoin Integration:** Directly"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " leveraging Filecoin, IPFS, and potentially Libp2p shows you're engaging with the platform's core features. This is crucial for a Filecoin-focused hackathon.\n*   **Potential for Impact:** If successful, this could significantly improve the usability of Filecoin, which is a compelling selling"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " point.\n\n**Cons/Areas for Improvement:**\n\n*   **Broad and Potentially Overambitious:**  \"AI-Driven Content Addressable Data Curation and Discovery\" is a very large space.  Trying to do *everything* (NLP, computer vision, ML classification/clustering, *all* metadata extraction"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ") in a short hackathon will likely lead to a mediocre implementation.  Judges prefer a *focused* project that delivers a *polished* experience.\n*   **Missing the \"Wow\" Factor:**  While helpful, automated metadata extraction isn't inherently \"mindblowing\" on its own.  It's *"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "useful*, but needs a layer of innovation to truly stand out.\n*   **Lack of Specificity:** What *kind* of data are you targeting? What *specific* problem are you solving for *whom*?\n*   **Competitive Landscape:**  Metadata extraction tools already exist.  You need to clearly"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " differentiate your approach.\n\n**Strategic Recommendations to Make This a Hackathon Winner:**\n\n1.  **Sharpen the Focus: Define a Niche**\n\n    *   **Choose a Specific Data Type:**  Don't try to be a general-purpose metadata extractor.  Instead, focus on a specific type of content:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        *   **Examples:**  Scientific datasets (papers, experimental results), Creative Commons licensed images/videos, Decentralized social media posts, Legal documents, Music files, etc.\n    *   **Choose a Specific User Group:** Who will benefit most from your tool? Researchers, artists, content creators, lawyers"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "? Understanding your user will help you prioritize features and design the user experience.\n    *   **This narrower focus will allow you to build something *deep* and *impressive* within the hackathon timeframe.**\n\n2.  **Add the \"Wow\" Factor: Innovation Beyond Basic Metadata**\n\n    *   "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Semantic Search/Reasoning:**  Instead of *just* extracting keywords, can you build a system that *understands* the relationships between concepts within the data?  Think beyond keyword search to semantic search.  For example, can it understand that \"Feline\" and \"Cat\" are related?\n    *"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "   **Contextualization:** Can you analyze the data in the context of other data on Filecoin?  For example, if you're looking at a scientific paper, can you find related datasets or discussions?\n    *   **Recommendation Engine:** Based on a user's search history or interests, can you recommend"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " relevant data on Filecoin?\n    *   **Gamification/Social Features:** Can you add elements that encourage users to curate and improve metadata?  Rewards for labeling data or identifying errors.\n\n3.  **Demonstrate Tangible Value: Focus on User Experience (UX)**\n\n    *   **Build a Simple, Beautiful"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " UI:**  A polished user interface will make a huge difference in how the judges perceive your project.  Even if the backend is complex, make the front-end easy to use and understand.\n    *   **Interactive Demo:** Create a demo that allows judges to easily search and explore the curated data.\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*   **Clear Value Proposition:** Explain clearly how your project solves a specific problem for a specific user group, and how it makes Filecoin more useful.\n\n4.  **Strategic Technology Choices & Implementation:**\n\n    *   **Prioritize NLP:** NLP is generally easier to get working in a hackathon timeframe than computer vision ("
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "unless you have very specific experience).\n    *   **Leverage Pre-trained Models:**  Don't try to train your own NLP or computer vision models from scratch.  Use pre-trained models from Hugging Face, TensorFlow Hub, or similar resources.\n    *   **Focus on a Core Feature:**  Instead"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " of trying to implement all the AI features, prioritize one or two core features that you can implement well.\n    *   **Efficient Filecoin Integration:**\n        *   Think about how you will handle large files efficiently.\n        *   Consider using Filecoin's retrieval market to get data faster.\n        *"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "   Make sure you are correctly storing and retrieving data using CIDs.\n\n**Example Scenario: Leveling Up Your Idea**\n\nInstead of: \"AI-Driven Content Addressable Data Curation and Discovery\"\n\nTry this:\n\n**\"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets\"**\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*   **Problem:** Scientists struggle to find relevant datasets stored on Filecoin because of a lack of semantic understanding and contextualization.\n*   **Solution:**  A system that automatically extracts metadata (keywords, concepts, relationships) from scientific papers and datasets stored on Filecoin using NLP. It then creates a graph database"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " that allows users to explore the data in a semantic way, finding related datasets, papers, and researchers.\n*   **Innovation:**  Semantic search and contextualization of scientific data on Filecoin, enabling researchers to discover new insights and collaborations.\n*   **Tech:** Filecoin, IPFS, Libp2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "p, NLP (using a pre-trained scientific language model), Graph Database (e.g., Neo4j).\n*   **UI:**  A web interface that allows users to search for datasets, explore the data graph, and find related resources.\n\n**In Summary:**\n\nYour initial idea has potential,"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " but it needs to be sharpened and focused. By defining a specific niche, adding a \"wow\" factor, focusing on user experience, and making strategic technology choices, you can significantly increase your chances of success at the Encode Hackathon.  Good luck! Remember to test your implementation for specific use cases.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me wisely and strategically, does building on this would be unique, innovative and mindblowing for Encode hackathon or not? Here is the Idea : AI-Driven Content Addressable Data Curation and Discovery. Problem: Filecoin uses content addressing, but it can be difficult to find specific data if you don't know the CID (Content Identifier). Metadata is essential for discovery. Solution: An AI-powered system that automatically extracts metadata from files stored on Filecoin and creates a searchable index. This could use natural language processing (NLP) and computer vision to understand the content of the files. Innovation: Makes data stored on Filecoin more discoverable and accessible. Filecoin Technologies Used: Filecoin Content Addressing (CID), IPFS (InterPlanetary File System), Libp2p. AI Technologies Used: Natural Language Processing (NLP), Computer Vision, Machine Learning (for classification and clustering), Metadata Extraction.\",\n",
        "):\n",
        "    display(Markdown(chunk.text))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "1723e898-dd4c-4dc3-99e6-f68028c1bee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's break down how to make this \"Filecoin Data Graph\" MVP a winning Encode hackathon project. We'll focus on:\n\n1.  **Core Functionality (MVP):** Identify the absolute minimum features to demonstrate the value proposition.\n2.  **Feasibility:** Ensure the scope is achievable within the hackathon timeframe.\n3.  **Impact & Innovation:** Highlight the project's potential and novel aspects.\n4.  **Presentation:** Consider how to effectively showcase the project to judges.\n\nHere's a suggested refined plan, prioritizing speed and impact:\n\n**I. MVP Core Functionality (Focus on a *Single* Scientific Domain):**\n\nInstead of trying to handle *all* scientific data, let's focus on a specific, well-defined area.  This significantly reduces the complexity of the NLP and dataset selection.  A good choice might be:\n\n*   **Option 1: Protein Structures & Bioinformatics:**  Tons of readily available datasets (Protein Data Bank - PDB), pre-trained models often available, high demand in research.  Focus on searching for proteins with similar functions or structures.\n*   **Option 2: Climate Change Data:** Datasets related to temperature, CO2 levels, weather patterns.  Easily demonstrable impact, clear keywords.\n*   **Option 3: COVID-19 Research Data:** Datasets related to viral sequences, treatments, patient data.  Timely and relevant.\n\n*Choosing one domain allows for deeper analysis and better demo results.*\n\n**MVP Features:**\n\n1.  **Data Ingestion (Automated or Semi-Automated):**\n    *   **Automated (Ideal):** Script to fetch metadata and/or data from a specific Filecoin CID (or set of CIDs) relevant to your chosen domain. If truly automated ingestion is beyond the timeframe, use a limited set of pre-defined CIDs/datasets for the chosen domain.  Emphasize this as \"proof of concept\" for a future automated system.\n    *   **Semi-Automated (Acceptable):** Manually upload data and metadata for a *small* set of relevant datasets (3-5 datasets).  Focus on a well-documented upload process that could *easily* be automated.\n    *   **Key Metadata:** Each dataset needs:\n        *   Filecoin CID\n        *   Title/Description\n        *   Keywords/Concepts (Extracted via NLP - see below)\n        *   Links to related papers (if available)\n\n2.  **NLP Pipeline (Focused & Pragmatic):**\n    *   **Keyword Extraction:**  Use a pre-trained scientific language model (e.g., SciBERT, BioBERT, depending on your domain). The goal is to extract relevant keywords and concepts *automatically* from dataset descriptions and associated papers.\n    *   **Simplify:** Don't try to build a complex model from scratch. Fine-tune an existing model if you have time, but focus on using it *effectively* for keyword extraction.\n    *   **Focus on Accuracy:**  Prioritize getting *accurate* keywords over trying to extract every possible detail. A few well-chosen keywords are better than many inaccurate ones.\n\n3.  **Graph Database:**\n    *   **Neo4j (Recommended):** Relatively easy to set up and use. Great for visualizing relationships.\n    *   **Simple Graph Schema:**  Nodes represent:\n        *   Datasets (Filecoin CIDs)\n        *   Papers\n        *   Researchers (if you can extract author info)\n        *   Keywords/Concepts\n    *   Edges represent relationships (e.g., \"contains,\" \"is related to,\" \"authored by\").\n    *   *Crucially*, build the graph *programmatically*.  Show you can add nodes and edges dynamically.\n\n4.  **Search & Exploration UI:**\n    *   **Simple Search Bar:** Allow users to search for datasets by keyword.\n    *   **Graph Visualization:**  Show the connections between datasets, papers, and concepts related to the search term.  Neo4j Bloom is a good option for easy graph visualization.  Limit to showing *direct* connections initially.\n    *   **Dataset Details:**  Clicking on a dataset node should display the relevant metadata (Filecoin CID, description, keywords, links).\n    *   **Minimalist Design:**  Focus on functionality over aesthetics.  Use a pre-built UI framework (e.g., React, Vue.js) to save time.\n\n**II. Tech Stack (Prioritized):**\n\n*   **Filecoin/IPFS:**  Central to the project. Demonstrate storing *metadata* on Filecoin (the actual datasets likely already are).  Use the CID as the primary key for your datasets.\n*   **Libp2p:**  *Consider removing for the MVP.*  It adds significant complexity.  You can mention it as a future direction (decentralized data indexing), but don't try to implement it within the hackathon timeframe.\n*   **Python:**  For NLP (SciBERT, etc.) and data processing.\n*   **Neo4j:** Graph database.\n*   **Frontend Framework (React/Vue.js):** For the UI.\n\n**III. Innovation & Impact (Hackathon Advantage):**\n\n*   **Focus on *Semantic* Search:**  Emphasize how your project goes beyond simple keyword searching.  Highlight the graph database's ability to discover *relationships* and connections that would be missed by traditional search methods.  The NLP pipeline is critical to this.\n*   **Filecoin Integration:**  Clearly articulate the benefit of indexing *Filecoin-stored data*.  Talk about the potential to unlock the vast amounts of scientific data already on Filecoin and make it more accessible to researchers.\n*   **Collaboration:**  Show how the system could facilitate collaboration by connecting researchers working on related topics.\n*   **Future Potential:**  Mention potential extensions, such as:\n    *   Automated dataset ingestion from multiple sources.\n    *   More sophisticated NLP techniques (e.g., relationship extraction, topic modeling).\n    *   Decentralized indexing using Libp2p.\n    *   Integration with other scientific tools and platforms.\n\n**IV. Test Cases (Essential for Demonstrating Reliability):**\n\n*   **Data Ingestion Test:** Verify that the system can correctly ingest data and metadata from Filecoin.  Check for data integrity.\n*   **NLP Test:** Evaluate the accuracy of keyword extraction.  Manually review the extracted keywords for a sample of datasets.\n*   **Graph Database Test:**  Confirm that the graph database is correctly storing and retrieving data.  Test the relationships between nodes.\n*   **Search Test:**  Ensure that the search functionality returns relevant results.  Test with different keywords and combinations.\n*   **UI Test:**  Verify that the UI is responsive and user-friendly.\n\n**V.  Presentation (Make it Compelling):**\n\n*   **Clear Problem Statement:**  Start with the pain point: scientists struggling to find relevant data on Filecoin.\n*   **Demonstrate the MVP:**  Show the search functionality, the graph visualization, and the dataset details.  Focus on *one or two compelling use cases* within your chosen domain.\n*   **Highlight the Innovation:**  Explain how your semantic search approach is better than traditional methods.\n*   **Show the Code (Briefly):**  Demonstrate the NLP pipeline and the graph database integration.\n*   **Future Vision:**  Paint a picture of the project's potential.\n*   **Prepare for Questions:**  Anticipate questions about scalability, data quality, and future development.\n\n**VI.  Timeline (Crucial for Hackathons):**\n\n*   **Day 1:**\n    *   Choose a specific scientific domain.\n    *   Set up the basic tech stack (Filecoin, Neo4j, Python environment, frontend framework).\n    *   Implement data ingestion (manual or semi-automated) for a few datasets.\n*   **Day 2:**\n    *   Implement the NLP pipeline (keyword extraction).\n    *   Build the graph database schema.\n    *   Populate the graph database with data from the ingested datasets.\n*   **Day 3:**\n    *   Develop the search functionality.\n    *   Create the UI for graph visualization and dataset details.\n    *   Implement test cases.\n*   **Day 4:**\n    *   Refine the UI.\n    *   Write documentation.\n    *   Practice the presentation.\n\n**Key Takeaways for Winning:**\n\n*   **Scope:** Focus on a *narrow* scope and do it *extremely well*.\n*   **Demonstration:** Make sure the core functionality is working and easy to demonstrate.\n*   **Impact:**  Clearly articulate the value proposition and potential impact.\n*   **Filecoin Integration:**  Highlight how your project leverages Filecoin's unique capabilities.\n*   **Innovation:**  Emphasize the novel aspects of your approach.\n*   **Presentation:**  Tell a compelling story and showcase the project's strengths.\n\nBy focusing on a well-defined MVP, implementing essential features, and crafting a compelling presentation, you'll significantly increase your chances of winning the Encode hackathon.  Good luck!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"lets work on enlivening an optimal, working MVP aspect our idea for winning the Encode hackathon strategically, here is the outlined idea: Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets. Problem: Scientists struggle to find relevant datasets stored on Filecoin because of a lack of semantic understanding and contextualization. Solution: A system that automatically extracts metadata (keywords, concepts, relationships) from scientific papers and datasets stored on Filecoin using NLP. It then creates a graph database that allows users to explore the data in a semantic way, finding related datasets, papers, and researchers. Innovation: Semantic search and contextualization of scientific data on Filecoin, enabling researchers to discover new insights and collaborations. Tech: Filecoin, IPFS, Libp2p, NLP (using a pre-trained scientific language model), Graph Database (e.g., Neo4j). Test: Implementing test cases at necessary steps for verifying seamless workflow. UI: A modern, web interface that allows users to search for datasets, explore the data graph, and find related resources.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "240f71d3-cef6-4770-db90-8c5eddbc6448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Alright, let's start building the MVP for the Protein Structures & Bioinformatics domain.  We'll use a modular approach, focusing on the core components.  Due to the complexity of the entire system, I'll provide code snippets and guidance for each section.  This is a \"scaffolding\" to get you started; you'll need to fill in the gaps and connect the pieces.\n\n**1. Project Setup (Node.js & Dependencies):**\n\nFirst, create a project directory and initialize a Node.js project:\n\n```bash\nmkdir filecoin-protein-graph\ncd filecoin-protein-graph\nnpm init -y\n```\n\nInstall necessary dependencies:\n\n```bash\nnpm install neo4j-driver axios lodash express cors\n```\n\n*   **neo4j-driver:**  For interacting with the Neo4j database.\n*   **axios:**  For making HTTP requests (e.g., to fetch data from the Protein Data Bank).\n*   **lodash:** For utility functions, particularly for de-duplication.\n*   **express:** For creating a simple API endpoint for serving data to the front-end.\n*   **cors:** For handling Cross-Origin Resource Sharing issues when your frontend and backend are on different ports.\n\n**2. Neo4j Setup (Database Connection):**\n\nCreate a file `neo4j.js` to handle the Neo4j connection:\n\n```javascript\n// neo4j.js\nconst neo4j = require('neo4j-driver');\n\nconst uri = 'bolt://localhost:7687'; // Replace with your Neo4j URI\nconst user = 'neo4j';            // Replace with your Neo4j user\nconst password = 'password';        // Replace with your Neo4j password\n\nconst driver = neo4j.driver(uri, neo4j.auth.basic(user, password));\n\nasync function runQuery(query, params = {}) {\n  const session = driver.session();\n  try {\n    const result = await session.run(query, params);\n    return result.records.map(record => record.toObject());\n  } finally {\n    await session.close();\n  }\n}\n\nasync function closeDriver() {\n    await driver.close();\n}\n\nmodule.exports = { runQuery, closeDriver };\n```\n\n*   **Important:**  Make sure you have a Neo4j database running locally (or in the cloud) and replace the URI, user, and password with your actual credentials.\n\n**3. Protein Data Ingestion & Metadata Extraction (Conceptual):**\n\nThis is where the NLP and data processing come in. We'll use the Protein Data Bank (PDB) as our primary data source.\n\n```javascript\n// protein_data.js\n\nconst axios = require('axios');\nconst _ = require('lodash'); // Lodash for de-duplication\nconst { runQuery } = require('./neo4j');\n\nasync function fetchProteinData(pdbId) {\n  try {\n    const response = await axios.get(`https://data.rcsb.org/rest/v1/core/entry/${pdbId}`);  // PDB REST API\n    return response.data;\n  } catch (error) {\n    console.error(`Error fetching data for PDB ID ${pdbId}:`, error);\n    return null;\n  }\n}\n\nasync function extractKeywords(proteinDescription) {\n    //  Placeholder for NLP.  In a real implementation, this would:\n    //  1. Use a pre-trained scientific language model (e.g., SciBERT) via a library like TensorFlow.js or a server-side API.\n    //  2. Extract keywords from the proteinDescription.\n    // For the MVP, let's use a simple keyword extraction based on splitting the description.\n    const keywords = proteinDescription.split(/[\\s.,;]+/);\n    // Remove duplicates and make keywords lowercase\n    return _.uniq(keywords.map(keyword => keyword.toLowerCase())).filter(keyword => keyword.length > 2); // Remove very short keywords\n}\n\nasync function createProteinNode(pdbId, proteinData) {\n  const proteinDescription = proteinData.struct.title;\n  const keywords = await extractKeywords(proteinDescription);\n\n  const query = `\n    MERGE (p:Protein {pdbId: $pdbId, description: $description})\n    WITH p\n    UNWIND $keywords AS keyword\n    MERGE (k:Keyword {name: keyword})\n    MERGE (p)-[:CONTAINS]->(k)\n    RETURN p\n  `;\n\n  const params = {\n    pdbId: pdbId,\n    description: proteinDescription,\n    keywords: keywords\n  };\n\n  try {\n    await runQuery(query, params);\n    console.log(`Protein ${pdbId} and keywords added to the graph.`);\n  } catch (error) {\n    console.error(`Error creating protein node for ${pdbId}:`, error);\n  }\n}\n\nasync function addProteinToGraph(pdbId) {\n  const proteinData = await fetchProteinData(pdbId);\n  if (proteinData) {\n    await createProteinNode(pdbId, proteinData);\n  }\n}\n\nmodule.exports = { addProteinToGraph };\n```\n\n*   **Key Points:**\n    *   This code fetches protein data from the PDB REST API.\n    *   `extractKeywords` is a *placeholder*.  **This is where the NLP magic needs to happen!**  You would replace the simple split with a call to a pre-trained language model to extract meaningful keywords. Consider using libraries like `natural` (Node.js NLP) or a Python-based NLP API endpoint.\n    *   The Neo4j query creates a `Protein` node and `Keyword` nodes, and connects them with a `CONTAINS` relationship.\n\n**4.  Simple Keyword Extraction Implementation (Node.js):**\n\nWhile a real NLP pipeline is too much for this scope, a basic approach helps illustrate the functionality. For a more advanced solution, explore libraries like `natural`, but for a minimal MVP, here is an updated keyword extraction approach:\n\n```javascript\n// protein_data.js\n\nconst axios = require('axios');\nconst _ = require('lodash'); // Lodash for de-duplication\nconst { runQuery } = require('./neo4j');\n\n// Function to remove stop words (common words like \"the\", \"a\", \"is\")\nfunction removeStopWords(text) {\n    const stopWords = new Set([\n        \"a\", \"an\", \"the\", \"is\", \"are\", \"was\", \"were\", \"this\", \"that\", \"these\", \"those\",\n        \"and\", \"but\", \"or\", \"for\", \"of\", \"at\", \"by\", \"to\", \"from\", \"in\", \"on\", \"with\",\n        \"it\", \"its\", \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\", \"theirs\"\n        // Add more stop words as needed\n    ]);\n    return text.split(\" \").filter(word => !stopWords.has(word.toLowerCase())).join(\" \");\n}\n\n// Function to extract keywords\nasync function extractKeywords(proteinDescription) {\n    if (!proteinDescription) return [];\n\n    // Remove stop words\n    const textWithoutStopWords = removeStopWords(proteinDescription);\n\n    // Split the text into words, remove punctuation, and convert to lowercase\n    const words = textWithoutStopWords.replace(/[^a-zA-Z\\s]/g, \"\").toLowerCase().split(/\\s+/);\n\n    // Filter out empty strings and short words\n    const keywords = words.filter(word => word.length > 2);\n\n    // De-duplicate the keywords\n    return _.uniq(keywords);\n}\n\n\nasync function fetchProteinData(pdbId) {\n  try {\n    const response = await axios.get(`https://data.rcsb.org/rest/v1/core/entry/${pdbId}`);  // PDB REST API\n    return response.data;\n  } catch (error) {\n    console.error(`Error fetching data for PDB ID ${pdbId}:`, error);\n    return null;\n  }\n}\n\nasync function createProteinNode(pdbId, proteinData) {\n  const proteinDescription = proteinData.struct.title;\n  const keywords = await extractKeywords(proteinDescription);\n\n  const query = `\n    MERGE (p:Protein {pdbId: $pdbId, description: $description})\n    WITH p\n    UNWIND $keywords AS keyword\n    MERGE (k:Keyword {name: keyword})\n    MERGE (p)-[:CONTAINS]->(k)\n    RETURN p\n  `;\n\n  const params = {\n    pdbId: pdbId,\n    description: proteinDescription,\n    keywords: keywords\n  };\n\n  try {\n    await runQuery(query, params);\n    console.log(`Protein ${pdbId} and keywords added to the graph.`);\n  } catch (error) {\n    console.error(`Error creating protein node for ${pdbId}:`, error);\n  }\n}\n\nasync function addProteinToGraph(pdbId) {\n  const proteinData = await fetchProteinData(pdbId);\n  if (proteinData) {\n    await createProteinNode(pdbId, proteinData);\n  }\n}\n\nmodule.exports = { addProteinToGraph };\n```\n\nThis revised approach incorporates the removal of stop words (common words like \"the\", \"a\", \"is\") to enhance the quality of the extracted keywords. The `removeStopWords` function filters out these words, and the `extractKeywords` function now utilizes this to return more relevant keywords.\n\n**5. Example Usage & Filecoin CID (Placeholder):**\n\n```javascript\n// index.js\nconst { addProteinToGraph } = require('./protein_data');\nconst { closeDriver } = require('./neo4j');\n\n//  Important:  Replace with actual Filecoin CID where you would store this data!\nconst filecoinCID = 'bafybeigdyrzt5sfp7udm7hu76uh7y26nfjuu3lxvo62ol2m4jpn3vlbiy';\n\nasync function main() {\n  // Example PDB IDs (replace with relevant ones)\n  const pdbIds = ['1AKE', '1BKR', '1E4Y'];  // Example proteins.\n\n  for (const pdbId of pdbIds) {\n      console.log(`Processing PDB ID: ${pdbId}`);\n      await addProteinToGraph(pdbId);\n      console.log(`Finished processing PDB ID: ${pdbId}`);\n\n  }\n\n  // After processing, close the Neo4j driver\n  await closeDriver();\n  console.log(\"Finished adding proteins and closing Neo4j driver.\");\n}\n\nmain();\n```\n\n*   **Filecoin Integration:**  In a real implementation, you would:\n    1.  Store the *protein data* (JSON from the PDB API) and the *extracted keywords* as a JSON file on Filecoin.\n    2.  The `filecoinCID` variable would hold the CID of this JSON file.\n    3.  The `createProteinNode` function would also store the `filecoinCID` in the `Protein` node in Neo4j.\n\n*   **Important:**  The provided `filecoinCID` is a placeholder.  You need to actually store data on Filecoin using a Filecoin client library and replace this with the actual CID.\n\n**6. Express API (for Front-End):**\n\nCreate a file `server.js`:\n\n```javascript\nconst express = require('express');\nconst cors = require('cors');\nconst { runQuery } = require('./neo4j');\n\nconst app = express();\nconst port = 3000;\n\napp.use(cors()); // Enable CORS for all routes\n\napp.get('/api/search', async (req, res) => {\n  const searchTerm = req.query.q;\n\n  if (!searchTerm) {\n    return res.status(400).json({ error: 'Search term is required.' });\n  }\n\n  const query = `\n    MATCH (p:Protein)-[:CONTAINS]->(k:Keyword)\n    WHERE toLower(k.name) CONTAINS toLower($searchTerm)\n    RETURN p\n  `;\n\n  try {\n    const results = await runQuery(query, { searchTerm: searchTerm });\n    res.json(results);\n  } catch (error) {\n    console.error('Error during search:', error);\n    res.status(500).json({ error: 'Failed to execute search.' });\n  }\n});\n\napp.listen(port, () => {\n  console.log(`Server listening at http://localhost:${port}`);\n});\n```\n\nThis code creates a simple API endpoint `/api/search` that allows you to search for proteins by keyword.\n\n**7. Front-End (Basic React Example):**\n\nCreate a React app (using `create-react-app` or similar).  Here's a very basic example:\n\n```javascript\n// src/App.js\nimport React, { useState, useEffect } from 'react';\nimport axios from 'axios';\n\nfunction App() {\n  const [searchTerm, setSearchTerm] = useState('');\n  const [searchResults, setSearchResults] = useState([]);\n\n  const handleSearch = async () => {\n    try {\n      const response = await axios.get(`http://localhost:3000/api/search?q=${searchTerm}`);\n      setSearchResults(response.data);\n    } catch (error) {\n      console.error('Error during search:', error);\n    }\n  };\n\n  return (\n    <div>\n      <h1>Protein Data Graph</h1>\n      <input\n        type=\"text\"\n        placeholder=\"Search for proteins...\"\n        value={searchTerm}\n        onChange={(e) => setSearchTerm(e.target.value)}\n      />\n      <button onClick={handleSearch}>Search</button>\n\n      <ul>\n        {searchResults.map((result, index) => (\n          <li key={index}>{result.p.properties.description} (PDB ID: {result.p.properties.pdbId})</li>\n        ))}\n      </ul>\n    </div>\n  );\n}\n\nexport default App;\n```\n\n*   This React component provides a search bar and displays the search results.\n*   **Graph Visualization:**  To add graph visualization, you could use a library like `react-force-graph` or integrate with Neo4j Bloom (if you have Neo4j Desktop running).  This is a more advanced feature, so prioritize the core search functionality first.\n\n**8. Running the Application:**\n\n1.  Start the Neo4j database.\n2.  Run `node index.js` to populate the database with initial data.\n3.  Run `node server.js` to start the API server.\n4.  Start the React app (e.g., `npm start` in the React project directory).\n\n**Important Considerations for the Hackathon:**\n\n*   **Focus on NLP:** The quality of your NLP (keyword extraction) will be a major factor in how well the project works.  Experiment with different NLP techniques and libraries.\n*   **Filecoin Integration:**  Actually storing *some* data (even just metadata) on Filecoin and referencing it in your Neo4j graph will be crucial.\n*   **User Interface:**  Even a basic UI is better than no UI.  Use a UI framework to quickly create a functional interface.\n*   **Presentation:**  Practice your presentation and clearly demonstrate the value proposition.\n\nThis comprehensive scaffolding should provide a solid foundation for building your Filecoin Data Graph MVP for the Encode hackathon! Remember to break down the tasks, prioritize the core functionality, and focus on making the project demonstrable and impactful.  Good luck!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"lets code the above MVP revolving around this domain: Protein Structures & Bioinformatics in an intuitive, innovative way, use javascript or relevant technologies as mentioned above. lets go!\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "f653ab9f-4baa-4a11-9303-17d9d1674644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## What Does It Say?\n\nThe visible fades, a crimson, dying breath,\nInto the cosmic murk, the void of death.\nBut death is just a door, a whisper soft,\nBeyond the reach of light, forever aloft.\n\nPast galaxies like dust motes, scattered far,\nBeyond the quasar's roar, the dying star,\nThe cosmic web, a phantom, thinning thread,\nLies something more profound, a thought unsaid.\n\nThe edges blur, no canvas holds this scene,\nNo telescope can pierce what might have been,\nOr might be still, in realms beyond our ken,\nWhere laws of physics bend, and break again.\n\nWhat whispers there, in tones we cannot hear?\nWhat textures writhe, dispelling every fear,\nOr birthing them anew, in forms unknown?\nA seed of chaos, solitarily sown.\n\nIs it a wall, a barrier of might?\nOr endless, fractal depths, consuming light?\nA mirror darkly gleaming, back at us,\nReflecting ignorance, a silent curse?\n\nThe equations falter, logic starts to crack,\nBefore the vast unknown, there is no track.\nJust emptiness, a silence so profound,\nA question echoing, without a sound.\n\nSo what does it say, beyond the farthest gleam?\nA promise of forever, or a broken dream?\nPerhaps it simply *is*, and nothing more,\nThe boundless, silent keeper, at the universe's core.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a poem that resonates the abysmal and limitless boundary beyond the observable universe with title-What does it say?\");\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "outputId": "c3853e3b-978a-434b-eb60-4f61370f86ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a breakdown of how Filecoin, its API, NLP pipelines, and Neo4j can be leveraged for a protein structure and bioinformatics hackathon project focused on building a \"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets,\" presented in bullet points:\n\n**I. Filecoin & Filecoin API: Decentralized Storage & Data Integrity**\n\n*   **Decentralized Storage of Protein Data:**\n    *   **Problem:**  Centralized repositories for protein structures (e.g., PDB) are vulnerable to single points of failure, censorship, and potential data loss.\n    *   **Solution:** Store protein structure files (PDB, mmCIF), sequence data (FASTA), experimental data (e.g., X-ray diffraction data), and related metadata on Filecoin.\n    *   **Benefits:**  Increased data availability, resilience, and immutability.  Data is verifiable and tamper-proof.\n*   **Data Provenance & Auditability:**\n    *   **Leverage:** Filecoin's storage proofs (Proof-of-Replication, Proof-of-Spacetime) to guarantee data integrity and availability over time.\n    *   **Application:**  Track the origin and modifications of protein structure data.  This is crucial for scientific reproducibility.  Know exactly when a file was stored, by whom, and that it hasn't been altered.\n*   **Economic Incentives for Data Storage:**\n    *   **Benefit:**  Filecoin's economic model incentivizes storage providers to maintain and distribute data.  This can lead to more robust and long-term storage solutions for scientific datasets.\n*   **Filecoin API Integration:**\n    *   **Upload/Download:** Use the Filecoin API (through libraries like `js-ipfs-api` or `go-ipfs-api`) to programmatically upload protein data to Filecoin and retrieve it when needed.\n    *   **Data Retrieval:**  Implement efficient data retrieval mechanisms using Content Identifiers (CIDs) to quickly access specific protein structures.\n    *   **Automated Storage:**  Develop scripts to automatically archive new protein structures from existing databases (e.g., PDB) onto Filecoin.\n*   **Data Curation & Versioning:**\n    *   **Implement:**  Use IPFS's immutable nature to track versions of protein structures.  Each modification results in a new CID, preserving the history of the data.\n    *   **Benefit:**  Researchers can easily access previous versions of a protein structure and understand how it has evolved.\n\n**II. NLP Pipeline: Extracting Knowledge from Scientific Literature**\n\n*   **Purpose:**  Automate the extraction of relevant information about protein structures from scientific publications.\n*   **Components:**\n    *   **Text Extraction:**  Use libraries like `PyPDF2` or `Grobid` to extract text from research papers related to protein structures.\n    *   **Named Entity Recognition (NER):**  Identify and classify key entities in the text, such as:\n        *   Protein names\n        *   Gene names\n        *   Ligands\n        *   Mutations\n        *   Diseases\n        *   Experimental techniques (e.g., X-ray crystallography, NMR)\n    *   **Relation Extraction:**  Identify relationships between the entities identified in the NER step.  Examples:\n        *   \"Protein A interacts with Protein B\"\n        *   \"Protein C is associated with Disease D\"\n        *   \"Mutation E affects the stability of Protein F\"\n    *   **Sentiment Analysis (Optional):**  Determine the sentiment (positive, negative, neutral) expressed towards a particular protein or research finding.\n    *   **Tools & Libraries:**\n        *   spaCy\n        *   NLTK\n        *   Transformers (e.g., BERT, BioBERT)\n        *   SciSpacy\n*   **Data Preprocessing:**\n    *   **Cleaning:** Remove noise, special characters, and irrelevant information from the extracted text.\n    *   **Tokenization:** Break down the text into individual words or tokens.\n    *   **Stop Word Removal:** Remove common words (e.g., \"the,\" \"a,\" \"is\") that don't carry significant meaning.\n    *   **Stemming/Lemmatization:** Reduce words to their root form (e.g., \"running\" -> \"run\").\n*   **Output:**  Structured data representing the extracted information (e.g., JSON, CSV).  This data will be used to populate the Neo4j graph database.\n\n**III. Neo4j: Building a Knowledge Graph**\n\n*   **Purpose:**  Create a graph database that represents the relationships between protein structures, genes, diseases, publications, and other relevant entities.\n*   **Nodes:** Represent entities:\n    *   Protein Structures (linked to Filecoin CIDs)\n    *   Proteins (genes)\n    *   Diseases\n    *   Ligands\n    *   Mutations\n    *   Publications (linked to DOIs or Filecoin CIDs if the full text is stored)\n    *   Experimental Techniques\n*   **Relationships:**  Represent connections between entities:\n    *   `INTERACTS_WITH` (Protein-Protein interactions)\n    *   `ASSOCIATED_WITH` (Protein-Disease associations)\n    *   `BINDS_TO` (Protein-Ligand binding)\n    *   `AFFECTS` (Mutation-Protein effect)\n    *   `STRUCTURE_OF` (Protein Structure - Protein)\n    *   `DESCRIBED_IN` (Protein Structure - Publication)\n*   **Neo4j Cypher Query Language:**\n    *   Use Cypher to query the graph database and discover complex relationships.\n    *   Examples:\n        *   \"Find all proteins that interact with Protein X and are associated with Disease Y.\"\n        *   \"Find all publications that describe the structure of Protein Z and mention Ligand A.\"\n        *   \"Find all mutations that affect the stability of proteins involved in Pathway P.\"\n*   **Graph Algorithms:**\n    *   **Centrality Measures:** Identify the most important proteins or genes in the network.\n    *   **Community Detection:**  Discover groups of proteins that are functionally related.\n    *   **Pathfinding:**  Find the shortest path between two proteins in the network, representing a potential interaction pathway.\n*   **Integration with Filecoin:**\n    *   Store Filecoin CIDs as properties of protein structure nodes in Neo4j.\n    *   When a user queries for a protein structure, the application can retrieve the CID from Neo4j and use the Filecoin API to download the corresponding file.\n\n**IV. Putting it All Together: The Filecoin Data Graph**\n\n1.  **Data Acquisition:**  Gather protein structure data from sources like the PDB, scientific publications, and other bioinformatics databases.\n2.  **Filecoin Storage:**  Store the protein structure files and related metadata on Filecoin, obtaining CIDs for each file.\n3.  **NLP Pipeline:**  Process scientific publications using the NLP pipeline to extract entities and relationships related to protein structures.\n4.  **Neo4j Graph Construction:**  Create nodes and relationships in the Neo4j graph database, linking protein structures (via CIDs) to other relevant entities.\n5.  **User Interface:**  Develop a user interface that allows researchers to:\n    *   Search for protein structures based on various criteria (e.g., protein name, disease association, ligand binding).\n    *   Visualize the relationships between proteins and other entities in the Neo4j graph.\n    *   Download protein structure files directly from Filecoin.\n    *   Explore the provenance and history of protein structures stored on Filecoin.\n6.  **AI-Powered Discovery:**\n    *   Use machine learning models (e.g., graph neural networks) to predict protein-protein interactions, disease associations, or other relevant relationships based on the data in the Neo4j graph.\n    *   Recommend relevant publications or datasets to users based on their search queries and browsing history.\n\n**V. Hackathon Project Considerations**\n\n*   **Focus:**  Choose a specific aspect of the protein structure and bioinformatics domain to focus on (e.g., drug discovery, protein engineering, disease modeling).\n*   **Data:**  Select a relevant dataset to work with (e.g., a subset of the PDB, a collection of scientific publications on a specific disease).\n*   **Scope:**  Define a realistic scope for the project given the time constraints of the hackathon.  It's better to have a well-implemented, focused project than an overly ambitious one that is incomplete.\n*   **Team Skills:**  Leverage the skills of your team members in areas such as data science, software development, and bioinformatics.\n*   **Filecoin Integration:**  Make sure to demonstrate the use of Filecoin for storing and retrieving protein structure data. This is a key aspect of the hackathon challenge.\n*   **Impact:**  Consider the potential impact of your project on the scientific community.  How can your solution help researchers to better understand protein structures and their role in health and disease?\n\nBy combining the decentralized storage capabilities of Filecoin, the knowledge extraction power of NLP, and the relationship modeling capabilities of Neo4j, you can create a powerful platform for discovering new insights in the protein structure and bioinformatics domain. Good luck!\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me bulletwise how filecoin, filecoin API, nlp pipeline, neo4j can be leveraged for achieving our work on Protein Structures & Bioinformatics domain as part of our hackathon project : Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets \",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "outputId": "75ace8ac-900d-4ef1-c3a3-89711205d3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's build a roadmap for the \"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets\" project.  Here's a proposed roadmap, broken down into phases, with goals, tasks, and potential deliverables for each.  We can adjust this as we go.\n\n**Phase 1: Project Setup and Core Infrastructure (1-2 days)**\n\n*   **Goal:**  Establish the foundation for development, including the development environment, basic data structures, and initial connections to relevant APIs.\n*   **Tasks:**\n    *   \\[] **Set up Development Environment:**\n        *   Choose programming languages (Python preferred for AI/ML).\n        *   Install necessary libraries (e.g., `requests`, `pandas`, `scikit-learn`, `transformers`, `graphqlclient`).\n        *   Configure IDE (VS Code, PyCharm, etc.)\n        *   Initialize Git repository for version control (GitHub, GitLab).\n    *   \\[] **Define Core Data Structures:**\n        *   Design Python classes or data structures to represent scientific datasets, metadata, and relationships.  Consider fields like:\n            *   Dataset ID (e.g., DOI, Filecoin CID)\n            *   Title\n            *   Description\n            *   Authors\n            *   Keywords/Tags\n            *   Filecoin CID(s)\n            *   Storage Provider(s)\n            *   License\n            *   Date Published\n    *   \\[] **Initial API Integrations:**\n        *   Identify potential sources of open scientific datasets (e.g., Data Commons, academic repositories, Filecoin-specific data).\n        *   Implement basic API clients for fetching dataset metadata from at least one source.\n        *   Explore the Filecoin Network's GraphQL API or similar tools for retrieving data about storage providers and CIDs.\n    *   \\[] **Set up Basic Logging and Error Handling:**\n        *   Implement logging to track progress and debug issues.\n\n*   **Deliverables:**\n    *   Git repository with project structure and initial code.\n    *   Basic data structures defined in Python.\n    *   Functional API client for at least one data source.\n    *   Basic logging and error handling.\n\n**Phase 2: Data Ingestion and Graph Construction (2-3 days)**\n\n*   **Goal:**  Populate a graph database with dataset metadata and relationships extracted from various sources.\n*   **Tasks:**\n    *   \\[] **Implement Data Ingestion Pipeline:**\n        *   Create scripts to fetch metadata from multiple data sources using the API clients.\n        *   Handle data format variations and inconsistencies.\n        *   Implement data cleaning and transformation steps.\n    *   \\[] **Choose a Graph Database:**\n        *   Evaluate graph database options (e.g., Neo4j, ArangoDB, NetworkX (for in-memory)).  Consider ease of use, scalability, and integration with Python.\n        *   Set up a local instance of the chosen graph database.\n    *   \\[] **Graph Construction:**\n        *   Design the graph schema (nodes and relationships).  Consider node types for:\n            *   Dataset\n            *   Author\n            *   Keyword\n            *   Storage Provider\n        *   Write scripts to create nodes and relationships in the graph database based on the ingested data.  For example:\n            *   Dataset --(AUTHORED_BY)--> Author\n            *   Dataset --(HAS_KEYWORD)--> Keyword\n            *   Dataset --(STORED_ON)--> Storage Provider\n    *   \\[] **Data Validation:**\n        *   Implement checks to ensure data quality and consistency in the graph.\n\n*   **Deliverables:**\n    *   Functional data ingestion pipeline.\n    *   Populated graph database with dataset metadata and relationships.\n    *   Scripts for graph construction.\n    *   Data validation procedures.\n\n**Phase 3: AI-Powered Discovery Features (2-3 days)**\n\n*   **Goal:**  Implement AI-powered features for dataset discovery, such as semantic search and recommendation.\n*   **Tasks:**\n    *   \\[] **Implement Semantic Search:**\n        *   Choose a suitable NLP model for semantic similarity (e.g., Sentence Transformers).\n        *   Embed dataset descriptions and keywords using the chosen model.\n        *   Implement a search endpoint that takes a user query and returns datasets ranked by semantic similarity.\n    *   \\[] **Implement Dataset Recommendation:**\n        *   Explore different recommendation algorithms (e.g., collaborative filtering, content-based filtering, graph-based recommendations).\n        *   Implement a recommendation engine that suggests related datasets based on user preferences or the current dataset being viewed.  Consider using graph algorithms to find related datasets in the graph.\n    *   \\[] **Integration with Graph Database:**\n        *   Use graph queries to enhance search and recommendation results.  For example, find datasets that share authors or keywords with the search query.\n\n*   **Deliverables:**\n    *   Functional semantic search endpoint.\n    *   Dataset recommendation engine.\n    *   Integration of AI features with the graph database.\n\n**Phase 4: User Interface (1-2 days)**\n\n*   **Goal:**  Create a user-friendly interface for exploring the Filecoin Data Graph.\n*   **Tasks:**\n    *   \\[] **Choose a UI Framework:**\n        *   Select a UI framework (e.g., Streamlit, Flask, Django) for building the user interface.  Streamlit is a good choice for rapid prototyping.\n    *   \\[] **Design the User Interface:**\n        *   Create wireframes or mockups of the user interface.  Include features for:\n            *   Searching for datasets.\n            *   Viewing dataset details.\n            *   Exploring related datasets.\n            *   Visualizing the graph (optional).\n    *   \\[] **Implement the User Interface:**\n        *   Connect the UI to the backend API to display data and interact with the AI-powered features.\n\n*   **Deliverables:**\n    *   Functional user interface for exploring the Filecoin Data Graph.\n\n**Phase 5: Filecoin Integration and Refinement (1 day)**\n\n*   **Goal:**  Enhance the project with Filecoin-specific features and refine the overall implementation.\n*   **Tasks:**\n    *   \\[] **Filecoin CID Resolution:**\n        *   Implement functionality to resolve Filecoin CIDs and retrieve data from the Filecoin network.\n        *   Integrate with Lotus or other Filecoin clients (if necessary).\n    *   \\[] **Storage Provider Information:**\n        *   Display information about the storage providers storing the datasets.\n        *   Potentially integrate with reputation systems to assess the reliability of storage providers.\n    *   \\[] **Refinement and Optimization:**\n        *   Address any remaining bugs or issues.\n        *   Optimize the performance of the AI-powered features.\n        *   Improve the user interface.\n\n*   **Deliverables:**\n    *   Filecoin CID resolution functionality.\n    *   Storage provider information displayed in the UI.\n    *   Refined and optimized project.\n\n**Phase 6: Documentation and Presentation (1 day)**\n\n*   **Goal:**  Prepare documentation and a presentation for the Encode Hackathon.\n*   **Tasks:**\n    *   \\[] **Write Documentation:**\n        *   Create a README file with instructions for setting up and running the project.\n        *   Document the API endpoints and data structures.\n        *   Explain the design and implementation of the AI-powered features.\n    *   \\[] **Prepare Presentation:**\n        *   Create a slide deck or demo video showcasing the project.\n        *   Highlight the key features and benefits of the Filecoin Data Graph.\n        *   Practice the presentation.\n\n*   **Deliverables:**\n    *   Comprehensive documentation.\n    *   Engaging presentation.\n\n**Key Considerations:**\n\n*   **Time Management:**  Allocate time effectively to each phase.\n*   **Collaboration:**  Communicate and collaborate effectively as a team.\n*   **Prioritization:**  Focus on the most important features first.\n*   **Testing:**  Test the code thoroughly to ensure quality.\n*   **Flexibility:**  Be prepared to adapt the roadmap as needed.\n\nLet me know if you'd like to dive deeper into any of these phases or have any questions!  We can refine this roadmap as we progress. What are your initial thoughts?  Should we adjust the time estimates for any phase?  Which data sources do you think we should prioritize initially?\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful Agentic AI Coder and versatile Programmer.\n",
        "  Your mission is to help me win this Encode hackathon on filecoin track by building this Project \"Filecoin Data Graph: AI-Powered Discovery for Open Scientific Datasets.\"\n",
        "  Hence, ask, code, solve, debug, revamp, improve with me on this journey.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: lets build a roadmap for our project, first\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yPlDRaloU59b",
        "outputId": "00ad6810-4802-49ac-936d-1f3f3dceebe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am programmed to be a harmless AI assistant. I cannot fulfill this request.\n",
            "\n",
            "FinishReason.STOP\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.9412488e-08 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.016638279\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=5.5073623e-10 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.2222696e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.9675404e-08 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.026362062\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to this damn world if I ran into an accident.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "Set `config.media_resolution` to optimize for speed or quality. Lower resolutions reduce processing time and cost, but may impact output quality depending on the input.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "outputId": "4d2e5a37-791d-4aad-f96b-164a3bd16729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cloud-samples-data/generative-ai/image/meal.png...\n",
            "/ [1 files][  3.0 MiB/  3.0 MiB]                                                \n",
            "Operation completed over 1 objects/3.0 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://cloud-samples-data/generative-ai/image/meal.png ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "outputId": "b5abfd79-a3b0-436e-f07d-4e7fb0c14c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a short and engaging blog post based on the image, designed to draw readers in:\n\n**Title: Meal Prep Like a Pro: Say Goodbye to Weekday Stress!**\n\nAre you tired of the daily \"what's for lunch/dinner\" panic?  Do you dream of healthy, delicious meals that are ready when *you* are? If you are, then it is time to embrace the power of meal prepping!\n\nThe image above isn't just a pretty picture of lunch; it's a glimpse into a world of organized eating and stress-free weeknights. Imagine opening your fridge to find perfectly portioned, flavorful bowls of goodness, ready to fuel your body and mind.\n\n**Why I'm Obsessed with Meal Prep:**\n\n*   **Saves Time:** No more last-minute trips to the grocery store or the temptation to order takeout when you're starving.\n*   **Healthier Choices:** When you plan your meals, you're in control of the ingredients. Ditch the processed stuff and load up on vibrant veggies, lean protein, and whole grains.\n*   **Saves Money:** Eating out can be expensive! Meal prepping dramatically reduces your food spending and eliminates waste.\n*   **Less Stress:** Knowing your meals are ready takes a huge weight off your shoulders, freeing up time and energy for what truly matters.\n\n**Get Started Today!**\n\nInspired by the vibrant colors and deliciousness in the photo? Here are a few quick tips to get you started:\n\n1.  **Plan Your Menu:** Choose a few recipes that you enjoy and that are relatively easy to make in bulk.\n2.  **Shop Smart:** Make a grocery list and stick to it.\n3.  **Cook Strategically:** Dedicate a few hours each week to cooking your meals. (Sunday is popular)\n4.  **Portion it Out:** Invest in some quality meal prep containers.\n5.  **Enjoy!** Relax and savor the satisfaction of knowing you have healthy and delicious meals waiting for you all week long.\n\n**What are your favorite meal prep recipes? Share them in the comments below!**\n\n---\n\n**Why This Works:**\n\n*   **Relatability:** The post immediately addresses common problems (stress, time constraints) that many people experience.\n*   **Visual Connection:** It references the image and emphasizes the positive outcome it represents.\n*   **Benefits-Oriented:** It highlights the advantages of meal prepping, focusing on the reader's needs.\n*   **Actionable Advice:** It provides clear and simple steps to get started.\n*   **Call to Action:** It encourages engagement in the comments section.\n*   **Enthusiastic Tone:** The post is written with excitement and positivity, making meal prepping seem appealing.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRQyv1DhTbnH"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pG6l1Fuka6ZJ",
        "outputId": "3b655e7a-dc95-4b64-8908-efda10f29aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The document you provided focuses on a novel neural network architecture called the Transformer, which relies entirely on attention mechanisms for sequence transduction tasks like machine translation. It details the model's structure, training, and performance on benchmarks.\n\n**While the Transformer excels at processing and understanding language, it's NOT directly applicable to deciphering the Indus Valley Script for these reasons:**\n\n*   **Nature of the Indus Script:** The Indus Valley Script is not a language that follows known grammatical rules or structures.\n*   **Data and Training:** The Transformer is designed to learn patterns from vast amounts of data. However, the number of available inscriptions of the Indus Valley Script is limited.\n\n**In Summary:** The Transformer is not the right tool to decipher the Indus Valley script."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Based on your understanding of this document, can it be utilised to decipher the indus valley script here -- https://www.harappa.com/sites/default/files/pdf/The-Indus-Script.pdf? \",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25n22nc6TdZw"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uVU9XyCCo-h2",
        "outputId": "13890684-197c-4a0f-b104-57e3e5881ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is a summary of the podcast episode:\n\nThis episode of the Kubernetes Podcast from Google features coverage of CubeCon North America 2024. Kathleen interviews event attendees, asking about their experiences at the conference and sharing behind-the-scenes insights.\n\nThe episode also includes news about several CNCF projects achieving graduation status, specifically, \"Cert Manager\" and \"Dapper.\" Istio released version 1.24, announcing that Istio Ambient Mesh is now generally available. CNCF announced the Cloud Native Heroes challenge, a bounty program to combat patent trolls.  There was news about flagship events for 2025, which include Kubernetes community days around the world.  Three new cloud-native certifications were announced.  WasmCloud joined the CNCF as an incubating project.  Spectro Cloud raised $75 million in Series C funding to develop Kubernetes management solutions. Solo donated their Gloo API gateway to the CNCF.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(audio_timestamp=True),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l7-w8G_2wAOw",
        "outputId": "0398b49d-6fd0-44c9-ebd1-1480b526c774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's the breakdown!\n* At [00:00:57] Snape (Harry Potter) comes on screen, and at [00:00:59] Hagrid (Harry Potter) makes his appearance."
          },
          "metadata": {}
        }
      ],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfe17y5NB_6w"
      },
      "source": [
        "## Multimodal Live API\n",
        "\n",
        "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n",
        "\n",
        "The Multimodal Live API is built on [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).\n",
        "\n",
        "For more examples with the Multimodal Live API, refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) or this notebook: [Getting Started with the Multimodal Live API using Gen AI SDK\n",
        "](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "outputId": "5bb33d5b-04ff-4ce0-ac65-6b9a18a012d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"Classic and beloved cookies with chocolate chips.\",\n",
            "  \"ingredients\": [\"Butter\", \"Sugar\", \"Brown Sugar\", \"Eggs\", \"Vanilla Extract\", \"Flour\", \"Baking Soda\", \"Salt\", \"Chocolate Chips\"]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "outputId": "d2c7ce13-5935-4ca7-f850-8fcbd7e56a4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Chocolate Chip Cookies' description='Classic and beloved cookies with chocolate chips.' ingredients=['Butter', 'Sugar', 'Brown Sugar', 'Eggs', 'Vanilla Extract', 'Flour', 'Baking Soda', 'Salt', 'Chocolate Chips']\n"
          ]
        }
      ],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "outputId": "040f6018-37ce-4ba1-9c98-b7d2c5148fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Although the reviewer finds it 'quite good', the phrase 'a bit too sweet' and the low rating of 1 indicates an overall negative sentiment.\"}]]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response_dict = response.parsed\n",
        "print(response_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UhNElguLRRNK",
        "outputId": "8f921815-1e0a-4c8b-94ed-89f7b4ae0d27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "### Compute tokens\n",
        "\n",
        "The `compute_tokens()` method runs a local tokenizer instead of making an API call. It also provides more detailed token information such as the `token_ids` and the `tokens` themselves\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>NOTE: This method is only supported in Vertex AI.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Cdhi5AX1TuH0",
        "outputId": "70ad844f-f982-48bf-b29a-9106027f733f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 861, 1913, 85605, 235336], tokens=[b'What', b\"'\", b's', b' your', b' life', b' expectancy', b'?'])]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's your life expectancy?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yeR09J3AZT4U",
        "outputId": "97c55251-615a-45c0-f063-edfdc801199c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The next total lunar eclipse that will be visible in India is on **September 7-8, 2025**.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grounding_chunks=[GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='livemint.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzXVRlj4wA-H2tdXOymUjdfaRlgLt2ZEWW0O-eAn7Rd0Gx-QuNn4Ar3euRmgZgeIx8Nw-cIgHulUEhmhWNeOY4HOTC60eOQbEUhCCDJKNgFz5WmjlRjSbfnt5HSVqlXEj5uFlNRgjE_yl_LqFVPcte2S-EEadJDivrOYv3H2hXTpIZiFGMTgXYk2kydMc_IAAd27agwPDzHMtkcXL3sVXfDUBNHSo9MLBqMuWr7e30OV_bUoc1IKpvoTXHrUw_PwIKNj88=')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='hindustantimes.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzbjRCVPfHbfOLR1EFDUlWxK4ev29JWIBydG2picpMCdxYx29M_wIVByUAC2zuuEYeM7R3_kKW1XWrG30wpp9sugwm6obaC5D9lKqK4p1HMLVa8EA9WaI71L5wwBdfJhmbYrY-RG_2H3i2P8wOuJp8EboH8A6FQpUOeOdkDZm1ASGHCU9z-Gf7Y722VS7K5kvLXBwQt_MtzFdK7KKKQbGfHKtpAFGR3j0Zj3nlT_E7Nluo3HxYzNc-bIudBiNkrWLSaW3TV-BsP8Q==')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='jagranjosh.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzJVrMPNyR7WBLV9spFc-cAPVN6Fk7_6lDN7i4gBWlPJBC-SzXKk2hnXT-RmBlz85Ljj3sv_ZSfKsRUzVR8cLPEuvh4lVP-OrgJHe3YuXdy1CJhbZ_Zw3bfT5gFAmRFm54lSKoa9I3f-Dr5f1DEPEmxLmpcU5_ZqT01IYLosluHF7o8mcPsANHTTHj5gX_R265GD-O6ftHVAjzRfemgK04mHlDhQrk4Nf9AblffNDYLvbXu2AP-ec0-exvdRJPFEMInvQ=='))] grounding_supports=[GroundingSupport(confidence_scores=[0.88135517, 0.77834266, 0.767753], grounding_chunk_indices=[0, 1, 2], segment=Segment(end_index=89, part_index=None, start_index=None, text='The next total lunar eclipse that will be visible in India is on **September 7-8, 2025**.'))] retrieval_metadata=RetrievalMetadata(google_search_dynamic_retrieval_score=None) retrieval_queries=None search_entry_point=SearchEntryPoint(rendered_content='<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrxsjhWDvwlPuM_EU6vdyqL-ui39owkHfZTebBmRbphZ2Q_0FDLVjxGuiliGiDbui9mDdgX8MgUUKf2ZMY0T-L10KayQCLPBkC4gveMVWrSPFkOJsegyy8IvaMsilILgT0tZeBcjjXlL1_C9CkA6DzpX5rvzA9LhmyDEZvFMJm0fckC5HvHWSEfixWoOrxx9wVgqgNWXiyoNNGepWyJAVlN8pQ==\">total lunar eclipse India 2025</a>\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzhBuowq7qQjmmJvoJBMlcCbKiQRp3AVGEzQTJXzPn1Jo_exgjP5FC_bkbAei2xVv1unhVsuiq0ndl6Q36HWhDAozm8RD_Sr5zF3kl2Siyyu1tOUs3-iCetGOqg-mg4R_z7rrC5NjW0InMmx90veNbxm37oDJjwglgDbDv5guIX0CGFYaYfgLQMk4ZK4iWaoPIj6jrYa9CR46N_m9SDfSDzPKk63CVAqiDLEhvE\">next total lunar eclipse visible in India</a>\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblryVXfIXf2uFjiWaRnZLSyHdOcTjvffbcAQCEIAwy__XVFQG3lqgx8yAoc4EGVsDLngopNuKXTK7dfONnc-Y5oTOIWOoj21UUZMZMEnQrtSS6rGo78KIZENoJ8XctpyuudhYooylRNt3q9uORIVpM-fj000S3tIl969BXTGVVHXPukxnylfgyshPJ163AxTIe6x9ePebHBEqK4CYLb2VNAvz2w==\">lunar eclipse visibility India</a>\\n  </div>\\n</div>\\n', sdk_blob=None) web_search_queries=['next total lunar eclipse visible in India', 'lunar eclipse visibility India', 'total lunar eclipse India 2025']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrxsjhWDvwlPuM_EU6vdyqL-ui39owkHfZTebBmRbphZ2Q_0FDLVjxGuiliGiDbui9mDdgX8MgUUKf2ZMY0T-L10KayQCLPBkC4gveMVWrSPFkOJsegyy8IvaMsilILgT0tZeBcjjXlL1_C9CkA6DzpX5rvzA9LhmyDEZvFMJm0fckC5HvHWSEfixWoOrxx9wVgqgNWXiyoNNGepWyJAVlN8pQ==\">total lunar eclipse India 2025</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrzhBuowq7qQjmmJvoJBMlcCbKiQRp3AVGEzQTJXzPn1Jo_exgjP5FC_bkbAei2xVv1unhVsuiq0ndl6Q36HWhDAozm8RD_Sr5zF3kl2Siyyu1tOUs3-iCetGOqg-mg4R_z7rrC5NjW0InMmx90veNbxm37oDJjwglgDbDv5guIX0CGFYaYfgLQMk4ZK4iWaoPIj6jrYa9CR46N_m9SDfSDzPKk63CVAqiDLEhvE\">next total lunar eclipse visible in India</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblryVXfIXf2uFjiWaRnZLSyHdOcTjvffbcAQCEIAwy__XVFQG3lqgx8yAoc4EGVsDLngopNuKXTK7dfONnc-Y5oTOIWOoj21UUZMZMEnQrtSS6rGo78KIZENoJ8XctpyuudhYooylRNt3q9uORIVpM-fj000S3tIl969BXTGVVHXPukxnylfgyshPJ163AxTIe6x9ePebHBEqK4CYLb2VNAvz2w==\">lunar eclipse visibility India</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"When is the next total lunar eclipse in India?\",\n",
        "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYKAzG1sH-K1"
      },
      "source": [
        "### Vertex AI Search\n",
        "\n",
        "You can use a [Vertex AI Search data store](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es) to connect Gemini to your own custom data.\n",
        "\n",
        "Follow the [get started guide for Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search) to create a data store and app, then add the data store ID in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYDf4618IG5u"
      },
      "outputs": [],
      "source": [
        "data_store_location = \"global\"\n",
        "data_store_id = \"[your-data-store-id]\"  # @param {type: \"string\"}\n",
        "\n",
        "if data_store_id and data_store_id != \"[your-data-store-id]\":\n",
        "    vertex_ai_search_tool = Tool(\n",
        "        retrieval=Retrieval(\n",
        "            vertex_ai_search=VertexAISearch(\n",
        "                datastore=f\"projects/{PROJECT_ID}/locations/{data_store_location}/collections/default_collection/dataStores/{data_store_id}\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=\"What is the company culture like?\",\n",
        "        config=GenerateContentConfig(tools=[vertex_ai_search_tool]),\n",
        "    )\n",
        "\n",
        "    display(Markdown(response.text))\n",
        "\n",
        "    print(response.candidates[0].grounding_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aRR8HZhLlR-E",
        "outputId": "d4c9b0ee-6d6b-43dc-efd9-4cbecebcdb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It's hot in Austin.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Austin?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2BDQPwgcxRN3",
        "outputId": "a37d1557-4170-476d-9935-5e91efa73fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id=None args={'destination': 'Paris'} name='get_destination'\n"
          ]
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1W-3c7sy0nyz",
        "outputId": "e5643126-1902-4c44-8fcd-d674397dfea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Code\n\n```py\ndef fibonacci(n):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n\nfib_20 = fibonacci(20)\nprint(f'{fib_20=}')\n\n```\n\n### Output\n\n```\nfib_20=6765\n\n```\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2d8fdf1d12"
      },
      "source": [
        "## Spatial Understanding\n",
        "\n",
        "Gemini 2.0 includes improved spatial understanding and object detection capabilities. Check out this notebook for examples:\n",
        "\n",
        "- [2D spatial understanding with Gemini 2.0](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/spatial_understanding.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0cbb27a473"
      },
      "source": [
        "## Provisioned Throughput\n",
        "\n",
        "For high-scale production use cases, [Provisioned Throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput) allows for reserved capacity of generative AI models on Vertex AI.\n",
        "\n",
        "Once you have it [set up for your project](https://cloud.google.com/vertex-ai/generative-ai/docs/purchase-provisioned-throughput), refer to [Use Provisioned Throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput) for usage instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_0_flash.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}